{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5OPn4z3pCrK"
   },
   "source": [
    "**Student's name**\n",
    "\n",
    "Mathieu Ruch\n",
    "\n",
    "An Bui Duc Khanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFoU9Ou3pCrX"
   },
   "source": [
    "This aim of this course is to review the evolution of image processing tools from hand-crafted methods to deep learning algorithms. The semester is split into four labs :\n",
    "\n",
    "* **Lab 1** : Introduction to Image Processing Using Hand-Crafted Features\n",
    "* **Lab 2** : Object detection\n",
    "* **Lab 3** : Object tracking\n",
    "* **Lab 4** : Introduction to Deep Learning for image classification and generative model\n",
    "\n",
    "Let's start with the second chapter of this course!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFBxeCmzpCrb"
   },
   "source": [
    "# Chapter 2 : Object Detection and Recognition\n",
    "(*100 points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 499
    },
    "executionInfo": {
     "elapsed": 6510,
     "status": "error",
     "timestamp": 1619414776133,
     "user": {
      "displayName": "Rémy Gardier",
      "photoUrl": "",
      "userId": "05844193576466295019"
     },
     "user_tz": -120
    },
    "id": "tp-D82mnpCrc",
    "outputId": "e5f65343-6a7a-49c0-b631-1dfd4cd048b2"
   },
   "outputs": [],
   "source": [
    "# !pip install scikit-learn imageio\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import tarfile\n",
    "from utils import *\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7d9KXsApCrg"
   },
   "source": [
    "## 2.1 Template Matching\n",
    "(*40 points*)\n",
    "\n",
    "In this warm-up section, we will address the problem of detection and recognition using Template Matching. \n",
    "\n",
    "Template matching is a 'brute-force' algorithm for object recognition. The most basic method of template matching is to directly compare the grayscale images, without using edge detection. For example, if you were trying to detect, let's say a football, you will need to create a base template of the object. During the operation, the template matching algorithm would analyze the current image to find areas which are similar to the template. This basic approach is quite limited. For one thing, it is not robust to inconsistent changes in brightness within the image. If the template image has strong features, a feature-based approach may be considered; the approach may prove further useful if the match in the search image might be transformed in some fashion. For templates without strong features, or for when the bulk of the template image constitutes the matching image, a template-based approach may be effective. \n",
    "\n",
    "In the naive approach, the difference between the template and the matching area is computed pixel by pixel and used to calculate the overall error. It is possible to reduce the number of sampling points by reducing the resolution of the search and template images by the some factor and performing the operation on the resultant downsized images (multiresolution, or Pyramid (image processing)), providing a search window of data points within the search image so that the template does not have to search every viable data point or a combination of both.\n",
    "\n",
    "![Template Matching Sample](../data/templateMatch.jpeg)\n",
    "\n",
    "Template matching example. Left: Template image. Right: Input image with the resulting image highlighted.  \n",
    "\n",
    "### 2.1.1 Objectives\n",
    "\n",
    "In this section, we will explore the advantages and disadvantages of template matching method. However, in contrast with the previous Chapter, we will leave most of the implementation for the reader (i.e. you).\n",
    "\n",
    "The following section will introduce to the most common metrics used for the matching distance and how to are used in OpenCV. Your task will be to: \n",
    " * Implement each metric *by hand* \n",
    " * Compare the accuracy against the OpenCV method \n",
    " * Analyse and report your observations for each metric in 3 exercises and one mini-challenge.\n",
    "\n",
    "As the final exercise, you will be given a set of more \"challenging\" data examples where using what you *learned before*, you will be asked to detect several objects in the scene. Your resulting algorithm __should have the given set of inputs and outputs__.\n",
    "\n",
    "### 2.1.2 Distance, Minimums and Maximums\n",
    "\n",
    "The two (and pretty much only) important parts of the Naive Template Matching algorithm is the *distance transform*, i.e. the metric to know if we found a match or not, and the global minima detection. \n",
    "\n",
    "For an  Input image $I$ if size $W\\times H$, a template Image $T$ of size $w\\times h$; ($w<W, h<H$), the distance methods implemented in OpenCv are the following: \n",
    "\n",
    "* Mean Squared Difference Method=CV_TM_SQDIFF\n",
    "\n",
    "\\begin{equation*}\n",
    "R(x,y)= \\sum _{x',y'} (T(x',y')-I(x+x',y+y'))^2 \n",
    "\\end{equation*}\n",
    "\n",
    "* Normalized Mean Squared Difference Method=CV_TM_SQDIFF_NORMED\n",
    "\\begin{equation*}\n",
    "        R(x,y)= \\frac{\\sum_{x',y'} (T(x',y')-I(x+x',y+y'))^2}{\\sqrt{\\sum_{x',y'}T(x',y')^2 \\cdot \\sum_{x',y'} I(x+x',y+y')^2}}\n",
    "\\end{equation*}\n",
    "\n",
    "* Cross Correlation Method=CV_TM_CCORR\n",
    "\\begin{equation*}\n",
    "        R(x,y)= \\sum _{x',y'} (T(x',y') \\cdot I(x+x',y+y'))\n",
    "\\end{equation*}\n",
    "\n",
    "* Normalized Cross Correlation method=CV_TM_CCORR_NORMED\n",
    "\\begin{equation*}\n",
    "        R(x,y)= \\frac{\\sum_{x',y'} (T(x',y') \\cdot I(x+x',y+y'))}{\\sqrt{\\sum_{x',y'}T(x',y')^2 \\cdot \\sum_{x',y'} I(x+x',y+y')^2}}\n",
    "\\end{equation*}\n",
    "\n",
    "* Correlation Coefficient Method=CV_TM_CCOEFF\n",
    "\\begin{equation*}\n",
    "        R(x,y)= \\sum _{x',y'} (T'(x',y') \\cdot I'(x+x',y+y'))\n",
    "\\end{equation*}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{array}{l} T'(x',y')=T(x',y') - 1/(w \\cdot h) \\cdot \\sum _{x'',y''} T(x'',y'') \\\\ I'(x+x',y+y')=I(x+x',y+y') - 1/(w \\cdot h) \\cdot \\sum _{x'',y''} I(x+x'',y+y'') \n",
    "\\end{array}\n",
    "\\end{equation*}\n",
    "\n",
    "* Normalized Correlation Coefficient=CV_TM_CCOEFF_NORMED\n",
    "\\begin{equation*}\n",
    "        R(x,y)= \\frac{ \\sum_{x',y'} (T'(x',y') \\cdot I'(x+x',y+y')) }{ \\sqrt{\\sum_{x',y'}T'(x',y')^2 \\cdot \\sum_{x',y'} I'(x+x',y+y')^2} }\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "Please notice that the dimensions of the output image, $R$, will depend on how you handle the edges. The easiest way is to ser the return an output image of size $(W-w+1, H-h+1)$.\n",
    "\n",
    "After the function finishes the comparison, the resulting image will contain an image map with the obtained values. In OpenCV, the best matches can be found as global minimums or maximums (depending which matric you used) using the `minMaxLoc()` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqSZKhGUpCrj"
   },
   "source": [
    "**QUESTION** (/3)\n",
    "\n",
    "For each function listed above, indicate if the best matching position is located either in the local minimums or in the maximums. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**\n",
    "\n",
    "According to the openCV documentation, the best matching position is located in a local minimum for methods using TM_SQDIFF and in a local maximum for methods using TM_CCORR or TM_CCOEFF. So we have that:\n",
    "\n",
    "The best matching position is located in a minimum for:\n",
    "- Mean Squared Difference Method (CV_TM_SQDIFF)\n",
    "- Normalized Mean Squared Difference Method (CV_TM_SQDIFF_NORMED)\n",
    "\n",
    "The best matching position is located in a maximum for:\n",
    "- Cross Correlation Method (CV_TM_CCORR)\n",
    "- Normalized Cross Correlation method (CV_TM_CCORR_NORMED)\n",
    "- Correlation Coefficient Method (CV_TM_CCOEFF)\n",
    "- Normalized Correlation Coefficient(CV_TM_CCOEFF_NORMED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqZgXCBYpCrl"
   },
   "source": [
    "**QUESTION** (/3)\n",
    "\n",
    "Based on the template  `my_distance_fn()` in the next, implement the functions `sqdiff()`, `sqdiff_normed()`, `ccorr()`, `ccorr_normed()`, `ccoeff()` and `ccoeff_normed()` that takes as input a region of an image, and the template and returns the corresponding distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_distance_fn(image_patch, template):\n",
    "    \"\"\"\n",
    "    Calculate the distance between image_patch and template\n",
    "    :image_patch:       Input region of interest of the image.\n",
    "    :template:          The Template Image.\n",
    "    :return:            The dummy distance.\n",
    "    \"\"\"\n",
    "    dummy_distance = np.sum(image_patch.astype(np.float32) - template.astype(np.float32))\n",
    "    return  dummy_distance\n",
    "\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "def sqdiff(image_patch, template):\n",
    "    \"\"\"\n",
    "    Calculate the Sum of Squared Differences (SSD) between image_patch and template\n",
    "    :image_patch:       Input region of interest of the image.\n",
    "    :template:          The Template Image.\n",
    "    :return:            The distance between image_patch and template.\n",
    "    \"\"\"\n",
    "    diff = image_patch.astype(np.float32) - template.astype(np.float32)\n",
    "    ssd = np.sum(np.square(diff))\n",
    "    return ssd\n",
    "\n",
    "def sqdiff_normed(image_patch, template):\n",
    "    \"\"\"\n",
    "    Calculate the Normalized Sum of Squared Differences (NSSD) between image_patch and template\n",
    "    :image_patch:       Input region of interest of the image.\n",
    "    :template:          The Template Image.\n",
    "    :return:            The distance between image_patch and template.\n",
    "    \"\"\"\n",
    "    diff = image_patch.astype(np.float32) - template.astype(np.float32)\n",
    "    numerator = np.sum(np.square(diff))\n",
    "    \n",
    "    denominator = np.sqrt(np.sum(image_patch.astype(np.float32)**2) * np.sum(template.astype(np.float32)**2))\n",
    "    sqdiff_normed = numerator / denominator if denominator != 0 else np.inf\n",
    "    return sqdiff_normed\n",
    "    \n",
    "\n",
    "\n",
    "def ccorr(image_patch, template):\n",
    "    \"\"\"\n",
    "    Calculate the cross-correlation distance between image_patch and template\n",
    "    :image_patch:       Input region of interest of the image.\n",
    "    :template:          The Template Image.\n",
    "    :return:            The cross-correlation distance.\n",
    "    \"\"\"\n",
    "    # Compute cross-correlation distance\n",
    "    ccorr = np.sum(image_patch.astype(np.float32) * template.astype(np.float32))\n",
    "    return ccorr\n",
    "\n",
    "\n",
    "def ccorr_normed(image_patch, template):\n",
    "    \"\"\"\n",
    "    Calculate the normalized cross-correlation distance between image_patch and template\n",
    "    :image_patch:       Input region of interest of the image.\n",
    "    :template:          The Template Image.\n",
    "    :return:            The normalized cross-correlation distance.\n",
    "    \"\"\"\n",
    "    # Compute numerator and denominator of normalized cross-correlation distance\n",
    "    numerator = np.sum(image_patch.astype(np.float32) * template.astype(np.float32))\n",
    "    denominator = np.sqrt(np.sum(image_patch.astype(np.float32)**2) * np.sum(template.astype(np.float32)**2))\n",
    "    \n",
    "    # Compute normalized cross-correlation distance\n",
    "    ccorr_normed = numerator / denominator if denominator != 0 else 0\n",
    "    return ccorr_normed\n",
    "\n",
    "\n",
    "def ccoeff(image_patch, template):\n",
    "    \"\"\"\n",
    "    Calculates the cross-correlation between image_patch and template.\n",
    "    \"\"\"\n",
    "    sum_T = np.sum(template)\n",
    "    sum_I = np.sum(image_patch)\n",
    "    corr = np.sum((image_patch - (sum_I / image_patch.size)) * (template - (sum_T / template.size)))\n",
    "    return corr\n",
    "\n",
    "def ccoeff_normed(image_patch, template):\n",
    "    \"\"\"\n",
    "    Calculates the normalized cross-correlation between image_patch and template.\n",
    "    \"\"\"\n",
    "    sum_T = np.sum(template)\n",
    "    sum_I = np.sum(image_patch)\n",
    "    numerator = np.sum((image_patch - (sum_I / image_patch.size)) * (template - (sum_T / template.size)))\n",
    "    denominator = np.sqrt(np.sum(np.square(image_patch - (sum_I / image_patch.size))) * np.sum(np.square(template - (sum_T / template.size))))\n",
    "    ccoeff_normed = numerator / denominator if denominator != 0 else 0\n",
    "    return ccoeff_normed\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance functions returns a single value, but we would like the distance between the image and the template at each pixel of the image. To do that, we use a sliding window approach which consists in selecting a region of interest in the image, calculate the distance between this region of interest and the template, and move to the next region of interest. In our case, the regions of interest are all regions of the same size of the template. We start at the pixel (0, 0) and move one pixel by one pixel.\n",
    "\n",
    "**QUESTION** (/3)\n",
    "\n",
    "In the next cell, implement the function `template_matching_process()` that takes as input an image, the template, and the distance function and returns the corresponding distance map between the image and the template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def template_matching_process(image, template, dist_fn):\n",
    "    \"\"\"\n",
    "    Given an input image, iterates over the image and computes the distance w/r\n",
    "    the template, using a given distance function. \n",
    "\n",
    "    :input_image:       Input image. :) \n",
    "    :template:          The Template Image.\n",
    "    :distance_function: Function used to compute the distance. The function should receive a image patch \n",
    "                        and a template as inputs.\n",
    "    :return:            The distance map.\n",
    "    \"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    t_height, t_width = template.shape[:2]\n",
    "    dist_map = np.zeros((height - t_height + 1, width - t_width + 1), dtype=np.float32)\n",
    "    \n",
    "    for y in range(height - t_height + 1):\n",
    "        for x in range(width - t_width + 1):\n",
    "            image_patch = image[y:y+t_height, x:x+t_width]\n",
    "            dist_map[y, x] = dist_fn(image_patch, template)\n",
    "            \n",
    "\n",
    "            \n",
    "    if dist_fn.__name__ == 'sqdiff_normed':\n",
    "\n",
    "        dist_map[dist_map > 1] = 1\n",
    "\n",
    "\n",
    "    \n",
    "    return dist_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test cell\n",
    "img_gray = cv.imread('../data/space-invaders_1.jpg', 0)\n",
    "template = cv.imread('../data/template_0.png', 0)\n",
    "\n",
    "# calculate distance\n",
    "dist_map = template_matching_process(img_gray, template, sqdiff)\n",
    "\n",
    "# display\n",
    "plt.figure(figsize=(12.8, 8.2))\n",
    "plt.subplot(131)\n",
    "plt.imshow(img_gray, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('Image')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.imshow(template, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('Template')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.imshow(dist_map, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('Distance Map')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell tests your implementation with the distance function `sqdiff_normed()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gx7AcLHmpCrn",
    "outputId": "811f3c20-2ca9-47ef-a064-5309b86b2a38"
   },
   "outputs": [],
   "source": [
    "# load image\n",
    "img_gray = cv.imread('../data/space-invaders_1.jpg', 0)\n",
    "template = cv.imread('../data/template_0.png', 0)\n",
    "\n",
    "# calculate distance\n",
    "dist_map = template_matching_process(img_gray, template, sqdiff_normed)\n",
    "\n",
    "# display\n",
    "plt.figure(figsize=(12.8, 8.2))\n",
    "plt.subplot(131)\n",
    "plt.imshow(img_gray, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('Image')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.imshow(template, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('Template')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.imshow(dist_map, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('Distance Map')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxnkNiHtpCrp"
   },
   "source": [
    "**QUESTION**  (/3)\n",
    "\n",
    "Check your outputs by using the build-in functions in OpenCV to compute the maps for each of the methods implemented. Show in each cell: your map and the OpenCV map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MEI3T6cbpCrq",
    "outputId": "c6d56541-192e-48fe-ad29-60102213ec42"
   },
   "outputs": [],
   "source": [
    "# Example\n",
    "res = cv.matchTemplate(img_gray, template, cv.TM_SQDIFF_NORMED)\n",
    "# Display two example maps\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 7),squeeze=False)\n",
    "display_image(dist_map, axes=ax[0][0])\n",
    "ax[0][0].set_title(\"My Distance Map\")\n",
    "display_image(res, axes=ax[0][1])\n",
    "ax[0][1].set_title(\"OpenCv Distance Map\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ikrLMKOpCru"
   },
   "outputs": [],
   "source": [
    "# sqdiff()\n",
    "# YOUR CODE HERE\n",
    "\n",
    "res = cv.matchTemplate(img_gray, template, cv.TM_SQDIFF)\n",
    "dist_map = template_matching_process(img_gray, template, sqdiff)\n",
    "# Display two example maps\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 7),squeeze=False)\n",
    "display_image(dist_map, axes=ax[0][0])\n",
    "ax[0][0].set_title(\"My Distance Map\")\n",
    "display_image(res, axes=ax[0][1])\n",
    "ax[0][1].set_title(\"OpenCv Distance Map\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sqdiff_normed()\n",
    "# YOUR CODE HERE\n",
    "\n",
    "res = cv.matchTemplate(img_gray, template, cv.TM_SQDIFF_NORMED)\n",
    "dist_map = template_matching_process(img_gray, template, sqdiff_normed)\n",
    "\n",
    "# Display two example maps\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 7),squeeze=False)\n",
    "display_image(dist_map, axes=ax[0][0])\n",
    "ax[0][0].set_title(\"My Distance Map\")\n",
    "display_image(res, axes=ax[0][1])\n",
    "ax[0][1].set_title(\"OpenCv Distance Map\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ccorr()\n",
    "# YOUR CODE HERE\n",
    "\n",
    "res = cv.matchTemplate(img_gray, template, cv.TM_CCORR)\n",
    "dist_map = template_matching_process(img_gray, template, ccorr)\n",
    "\n",
    "# Display two example maps\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 7),squeeze=False)\n",
    "display_image(dist_map, axes=ax[0][0])\n",
    "ax[0][0].set_title(\"My Distance Map\")\n",
    "display_image(res, axes=ax[0][1])\n",
    "ax[0][1].set_title(\"OpenCv Distance Map\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ccorr_normed()\n",
    "# YOUR CODE HERE\n",
    "\n",
    "res = cv.matchTemplate(img_gray, template, cv.TM_CCORR_NORMED)\n",
    "dist_map = template_matching_process(img_gray, template, ccorr_normed)\n",
    "\n",
    "# Display two example maps\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 7),squeeze=False)\n",
    "display_image(dist_map, axes=ax[0][0])\n",
    "ax[0][0].set_title(\"My Distance Map\")\n",
    "display_image(res, axes=ax[0][1])\n",
    "ax[0][1].set_title(\"OpenCv Distance Map\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ccoeff()\n",
    "# YOUR CODE HERE\n",
    "\n",
    "res = cv.matchTemplate(img_gray, template, cv.TM_CCOEFF)\n",
    "dist_map = template_matching_process(img_gray, template, ccoeff)\n",
    "\n",
    "# Display two example maps\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 7),squeeze=False)\n",
    "display_image(dist_map, axes=ax[0][0])\n",
    "ax[0][0].set_title(\"My Distance Map\")\n",
    "display_image(res, axes=ax[0][1])\n",
    "ax[0][1].set_title(\"OpenCv Distance Map\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ccoeff_normed()\n",
    "# YOUR CODE HERE\n",
    "\n",
    "res = cv.matchTemplate(img_gray, template, cv.TM_CCOEFF_NORMED)\n",
    "dist_map = template_matching_process(img_gray, template, ccoeff_normed)\n",
    "\n",
    "# Display two example maps\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 7),squeeze=False)\n",
    "display_image(dist_map, axes=ax[0][0])\n",
    "ax[0][0].set_title(\"My Distance Map\")\n",
    "display_image(res, axes=ax[0][1])\n",
    "ax[0][1].set_title(\"OpenCv Distance Map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7Wa7im0pCrv"
   },
   "source": [
    "**QUESTION** (/2)\n",
    "\n",
    "Comment the differences between the OpenCV function and yours (speed, precision, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The openCV implementation is more precise than the one we implemented. Manually checking the minimum values in the OpenCV map, we see that it holds values as small as 1e-7, while our minimums are rounded down to 0. Moreover, the OpenCV implementation is faster than ours due to the use of optimized libraries and matrix manipulation functions, while we use for loops to iterate over the image and template matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KG_EUlh3pCrw"
   },
   "source": [
    "### 2.1.3 Finding Local Minimums / Maximums\n",
    "\n",
    "As explained before, in order to find the location of our possible object we need to find the minimum or maximum point in our resulting distance map. The  OpenCV function `cv.minMaxLoc()` can be used to find the local minimum and maximum of the single-channel array (1D or 2D) [[doc](https://docs.opencv.org/4.0.0/d2/de8/group__core__array.html#gab473bf2eb6d14ff97e89b355dac20707)]. \n",
    "\n",
    "However, if we want to detect several objects in the image, the function `cv.minMaxLoc()` won't give you all the locations. \n",
    "\n",
    "**QUESTION**  (/2)\n",
    "\n",
    "Implement your own function, `multiMinMax()`, which takes an input 2D image `src` and returns an `output_array` with the locations of the minimums or maximums depending on the provided `flag` (`flag = \"min\"` or `flag = \"max\"`), and a given `params`. The `params` variable determines the number of matching objects to return. It can be, for example, a _threshold_ for the local minima/maxima, the maximum number of maximums/minimums to return, a difference between the global maxima/minima to be included, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWK43sUIpCry"
   },
   "outputs": [],
   "source": [
    "def multiMinMax(src, flag, params):\n",
    "    \"\"\"\n",
    "    Return the extrema of an image\n",
    "    :src:       Image\n",
    "    :flag:      Minima or maxima\n",
    "    :param:  Parameters of your function\n",
    "    return: The minimum and maximum\n",
    "    \"\"\"\n",
    "    ret = None \n",
    "    \n",
    "    #######################\n",
    "\n",
    "    if flag not in [\"min\", \"max\"]:\n",
    "        return \"Invalid flag. Please choose 'min' or 'max'.\"\n",
    "\n",
    "    global_min, global_max, _, _ = cv.minMaxLoc(src)\n",
    "    # print(global_min, global_max)\n",
    "    if flag == \"min\":\n",
    "        diff_img = abs(global_min - src)/abs(global_max-global_min)\n",
    "    else:\n",
    "        diff_img = abs(src - global_max)/abs(global_max-global_min)\n",
    "    # print(diff_img.shape)\n",
    "    ret = np.where(diff_img <= params)\n",
    "    return ret\n",
    "    #######################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell tests your implementation and draws the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qh4bcCM1pCry"
   },
   "outputs": [],
   "source": [
    "def drawROIS(src, template, locations, color = (0,0,255)):\n",
    "    w, h = template.shape[::-1]\n",
    "    # Make copy of image to draw on it without changing the original image\n",
    "    canvas = src.copy();\n",
    "    \n",
    "    # Draw all rectangles\n",
    "    for pt in list(zip(*locations[::-1])):\n",
    "        # print(pt)\n",
    "        cv.rectangle(canvas, pt, (pt[0] + w, pt[1] + h), color, 2)\n",
    "       \n",
    "    return canvas\n",
    "\n",
    "# How should be called:\n",
    "img_rgb  = cv.imread('../data/space-invaders_1.jpg')\n",
    "img_gray = cv.imread('../data/space-invaders_1.jpg',0)\n",
    "template = cv.imread('../data/template_0.png',0)\n",
    "\n",
    "# For a given distance Map\n",
    "distance_map_norm_corr = cv.matchTemplate(img_gray,template,cv.TM_CCOEFF_NORMED)\n",
    "# Use your function here!\n",
    "locations = multiMinMax(distance_map_norm_corr,'max',6e-2)\n",
    "# print(locations)\n",
    "# Draw the ROIs \n",
    "img_rgb_holder = drawROIS(img_rgb, template, locations)\n",
    "# example:\n",
    "display_image(img_rgb_holder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = multiMinMax(distance_map_norm_corr,'max',1e-2)\n",
    "locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWyxxmoVpCrz"
   },
   "source": [
    "\n",
    "\n",
    "Now that you are all set up, use the functions you implemented and *try* to detect ALL the matching objects in the input image (Using template Matching).\n",
    "\n",
    "Rules:\n",
    "\n",
    "* You can use any metric you want, self-implemented or from OpenCV. \n",
    "* You can tweak your `multiMinMax()` to get better results.\n",
    "* For the points 1),2) and 3) below you have to do it in grayscale.\n",
    "* 4) can use multi-channel heuristics.\n",
    "\n",
    "\n",
    "#### 1) Perfect match <3\n",
    "\n",
    "The next cell load the image and the templates. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZotMFerpCr0"
   },
   "outputs": [],
   "source": [
    "# input images to use\n",
    "p1_src_rgb    = cv.imread('../data/space-invaders_1.jpg')\n",
    "p1_src_gray   = cv.imread('../data/space-invaders_1.jpg',0)\n",
    "\n",
    "# Show the matching of these 2 templates:\n",
    "p1_template_1 = cv.imread('../data/template_1.png',0)\n",
    "p1_template_2 = cv.imread('../data/template_2.png',0)\n",
    "\n",
    "# Image\n",
    "display_image(p1_src_gray)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(3, 1),squeeze=False)\n",
    "\n",
    "# Templates\n",
    "display_image(p1_template_1, axes=ax[0][0])\n",
    "ax[0][0].set_title(\"Template 1\")\n",
    "ax[0][0].set_xticks([])\n",
    "ax[0][0].set_yticks([])\n",
    "\n",
    "display_image(p1_template_2, axes=ax[0][1])\n",
    "ax[0][1].set_title(\"Template 2\")\n",
    "ax[0][1].set_xticks([])\n",
    "ax[0][1].set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION** (/4)\n",
    "\n",
    "Using the graycale template `p1_template_1` and `p1_template_2` provided in the previous cell, detect all the objects in the image `p1_src_gray` and show the locations of the matching objects in the image `p1_src_rgb`.\n",
    "\n",
    "Follow the code below and provide some insights like:\n",
    "\n",
    "* Why did you choose that given metric?  \n",
    "* How robust to false positives/negatives is your selected metric.\n",
    "* Is the number of output locations the same as the matching objects? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aIHmg4uQpCr0"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "# Perform template matching for both templates\n",
    "dist_map_template_1 = cv.matchTemplate(p1_src_gray, p1_template_1, cv.TM_CCOEFF_NORMED)\n",
    "dist_map_template_2 = cv.matchTemplate(p1_src_gray, p1_template_2, cv.TM_CCOEFF_NORMED)\n",
    "\n",
    "# Use multiMinMax function to get the locations of matching objects\n",
    "locations_template_1 = multiMinMax(dist_map_template_1, 'max', 6e-2)\n",
    "locations_template_2 = multiMinMax(dist_map_template_2, 'max', 6e-2)\n",
    "\n",
    "# Draw the ROIs on the RGB image for both templates\n",
    "img_rgb_with_ROIs_template_1 = drawROIS(p1_src_rgb, p1_template_1, locations_template_1)\n",
    "img_rgb_with_ROIs_template_2 = drawROIS(img_rgb_with_ROIs_template_1, p1_template_2, locations_template_2)\n",
    "\n",
    "# Display the image with detected objects\n",
    "display_image(img_rgb_with_ROIs_template_2)\n",
    "# print(len(locations_template_1[0]), len(locations_template_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onLb_kvdpCr2"
   },
   "source": [
    "#### 2) Not so perfect Match </3\n",
    "\n",
    "\n",
    "The next cell load the image and the templates. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-GZlYL6rpCr2"
   },
   "outputs": [],
   "source": [
    "# input\n",
    "p2_src_rgb    = cv.imread('../data/space-invaders_2.jpg')\n",
    "p2_src_gray   = cv.imread('../data/space-invaders_2.jpg',0)\n",
    "\n",
    "# 2 tempaltes (check that tempalte one don't match all the invaders in the same row) \n",
    "p2_template_1 = cv.imread('../data/template_3.png',0)\n",
    "p2_template_2 = cv.imread('../data/template_1.png',0)\n",
    "\n",
    "# Image\n",
    "display_image(p2_src_gray)\n",
    "\n",
    "# Templates\n",
    "fig, ax = plt.subplots(1, 2, figsize=(3, 1),squeeze=False)\n",
    "display_image(p2_template_1, axes=ax[0][0])\n",
    "ax[0][0].set_title(\"Template 1\")\n",
    "ax[0][0].set_xticks([])\n",
    "ax[0][0].set_yticks([])\n",
    "\n",
    "display_image(p2_template_2, axes=ax[0][1])\n",
    "ax[0][1].set_title(\"Template 2\")\n",
    "ax[0][1].set_xticks([])\n",
    "ax[0][1].set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION** (/4)\n",
    "\n",
    "Using the graycale template `p2_template_1` and `p2_template_2` provided in the previous cell, detect all the objects in the image `p2_src_gray` and show the locations of the matching objects in the image `p2_src_rgb`. All the _invaders_ in the same row counts as the \"same\" class. \n",
    "\n",
    "Follow the code bellow and provide some insights like:\n",
    "\n",
    "* How did you select the number of maximums/minimus?  \n",
    "* How robust to false positives/negatives is your selected metric.\n",
    "* Is the number of output locations the same as the matching objects? \n",
    "* Could you use any of the features from the last chapter to improve the matching?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yGNt-oMOpCr3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "# Perform template matching for both templates\n",
    "dist_map_template_1 = cv.matchTemplate(p2_src_gray, p2_template_1, cv.TM_CCOEFF_NORMED)\n",
    "dist_map_template_2 = cv.matchTemplate(p2_src_gray, p2_template_2, cv.TM_CCOEFF_NORMED)\n",
    "\n",
    "# Use multiMinMax function to get the locations of matching objects\n",
    "locations_template_1 = multiMinMax(dist_map_template_1, 'max', 2.5e-1)\n",
    "locations_template_2 = multiMinMax(dist_map_template_2, 'max', 3e-1)\n",
    "\n",
    "# Draw the ROIs on the RGB image for both templates\n",
    "img_rgb_with_ROIs_template_1 = drawROIS(p2_src_rgb, p2_template_1, locations_template_1)\n",
    "img_rgb_with_ROIs_template_2 = drawROIS(img_rgb_with_ROIs_template_1, p2_template_2, locations_template_2)\n",
    "\n",
    "# Display the image with detected objects\n",
    "display_image(img_rgb_with_ROIs_template_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Hidden objects game\n",
    "\n",
    "\n",
    "Finding hidden objects in cluttered illustration is a popular casual game which develop your observation skills. In this exercise, you will use your template matching skills to develop an efficient object detector for this game.\n",
    "\n",
    "\n",
    "We will play on scenes from the **Big Home Hidden Objects** game, in which image templates of the objects to find are shown at the bottom of the scene. We provide you the method `extract_big_home_templates()` to extract the grayscale template of each object given a scene.\n",
    "\n",
    "The next cell load the image and the templates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS FUNCTION\n",
    "def extract_big_home_templates(image):\n",
    "    \n",
    "    # We extract list of objects in a heuristic manner\n",
    "    list_image = image[370:418, 77:712].copy()\n",
    "    \n",
    "    # There are 10 objects per image\n",
    "    n_objects = 10\n",
    "    stride = list_image.shape[1]/10\n",
    "    \n",
    "    # Extract each template image\n",
    "    objects = []\n",
    "    for i in range(n_objects):\n",
    "        object_rgb = list_image[:,int(stride*i):int(stride*(i+1))][15:-15,15:-15]\n",
    "        center_coordinates = (int(object_rgb.shape[1]/2), int(object_rgb.shape[0]/2))\n",
    "        object_rgb = cv2.ellipse(object_rgb, center_coordinates, (40, 32), 0, 0, 360, (35, 35,35), 20)\n",
    "        object_gray = cv2.cvtColor(object_rgb, cv2.COLOR_BGR2GRAY)\n",
    "        objects.append(object_gray)\n",
    "        \n",
    "    return objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inputs\n",
    "hidden_rgb  = cv.imread('../data/hidden.jpeg')\n",
    "\n",
    "# get list of templates\n",
    "templates = extract_big_home_templates(hidden_rgb)\n",
    "hidden_rgb = hidden_rgb[80:360]\n",
    "hidden_gray = cv2.cvtColor(hidden_rgb, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10, 10),squeeze=False)\n",
    "\n",
    "display_image(hidden_rgb, axes=ax[0][0])\n",
    "ax[0][0].set_title(\"Input\")\n",
    "ax[0][0].set_xticks([])\n",
    "ax[0][0].set_yticks([])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1,10, figsize=(20, 20),squeeze=False)\n",
    "\n",
    "for i, template in enumerate(templates):\n",
    "    display_image(template, axes=ax[0][i])\n",
    "    ax[0][i].set_title(\"Templates {:d}\".format(i+1))\n",
    "    ax[0][i].set_xticks([])\n",
    "    ax[0][i].set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**QUESTION** (/10)\n",
    "\n",
    "Your task is to use these templates and what you have learned so far to find the hidden objects in the given illustration `hidden_gray`.\n",
    "\n",
    "Plot the image `hidden_rgb` showing the location of each object and report the number of miss. Note each object is present once in the scene. So you can write a variant of `multiMinMax()` (e.g `minMax()`) which returns the global minimum or maximum of a 2D `src` image according to the provided `flag` (`flag = \"min\"` or `flag = \"max\"`). You can reuse the function `cv.minMaxLoc()`.\n",
    "\n",
    "\n",
    "In this exercise, you may choose to use any transformation in the input image or tweak the distance metric. You can't modify the templates (only scale it).\n",
    "\n",
    "Follow the code below and provide some insights like:\n",
    "\n",
    "* What metric seemed to work better this time ? \n",
    "* Was it different from the previous exercise ?\n",
    "* How many objects did you find ? Why these ones in partiular ?\n",
    "\n",
    "Save the output in a separate image for easier visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minMax(src, flag):\n",
    "    \n",
    "    #YOUR CODE HERE\n",
    "    if flag not in [\"min\", \"max\"]:\n",
    "        return \"Invalid flag. Please choose 'min' or 'max'.\"\n",
    "    \n",
    "    global_min, global_max, min_loc, max_loc = cv.minMaxLoc(src)\n",
    "    \n",
    "    if flag == \"min\":\n",
    "        return (global_min, [[min_loc[1]], [min_loc[0]]])\n",
    "    else:\n",
    "        return (global_max, [[max_loc[1]], [max_loc[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n",
    "\n",
    "\n",
    "def find_objects(hidden_gray, hidden_rgb, templates):\n",
    "    found_objects = []\n",
    "    for i, template in enumerate(templates):\n",
    "        dist_map = cv.matchTemplate(hidden_gray, template, cv.TM_CCOEFF_NORMED)\n",
    "        val, loc = minMax(dist_map, 'max')\n",
    "        # max_loc = ([max_loc[1]], [max_loc[0]])\n",
    "        \n",
    "\n",
    "        found_objects.append((i, loc))\n",
    "        img_with_ROI = drawROIS(hidden_rgb, template, loc)\n",
    "        print(\"Found template\", i+1, \"at\", loc[::-1], \"with value\", val)\n",
    "        hidden_rgb = img_with_ROI.copy()\n",
    "    \n",
    "    return hidden_rgb, found_objects\n",
    "\n",
    "\n",
    "# Find the objects\n",
    "result_img, found_objects = find_objects(hidden_gray, hidden_rgb, templates)\n",
    "\n",
    "# Display the image with detected objects\n",
    "fig, ax = plt.subplots(1,1, figsize=(20, 20),squeeze=False)\n",
    "\n",
    "display_image(result_img, axes=ax[0][0])\n",
    "ax[0][0].set_title(\"Input\")\n",
    "ax[0][0].set_xticks([])\n",
    "ax[0][0].set_yticks([])\n",
    "\n",
    "plt.show()\n",
    "fig, ax = plt.subplots(1,10, figsize=(20, 20),squeeze=False)\n",
    "\n",
    "for i, template in enumerate(templates):\n",
    "    display_image(template, axes=ax[0][i])\n",
    "    ax[0][i].set_title(\"Templates {:d}\".format(i+1))\n",
    "    ax[0][i].set_xticks([])\n",
    "    ax[0][i].set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_RLLoJq1pCr5"
   },
   "source": [
    "#### 4) Find all hidden objects\n",
    "\n",
    "As you can see, using template matching can be tricky, even when you have _good_ templates. As for the last exercise, you will try to design good __templates__  in order to find all hidden objects in the image. In this exercise, you can (if you want) use the information of the 3 RGB channels (to generate a 1 _better_ 1 channel image for example) and transform the input image (for example to homogenize the scale!). You can reuse any features from the last chapter to improve the matching.\n",
    "\n",
    "**QUESTION** (/6)\n",
    "\n",
    "Plot your selected templates and the input image _showing_ the location of each object (if you can) and any miss if any.\n",
    "\n",
    "Follow the code below and provide some insights like:\n",
    "\n",
    "* Was it different from the previous exercise?\n",
    "* What did you improve ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "hidden_rgb  = cv.imread('../data/hidden.jpeg')\n",
    "\n",
    "# get list of templates\n",
    "hidden_rgb = hidden_rgb[80:360]\n",
    "hidden_gray = cv2.cvtColor(hidden_rgb, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10, 10),squeeze=False)\n",
    "\n",
    "display_image(hidden_rgb, axes=ax[0][0])\n",
    "ax[0][0].set_title(\"Input\")\n",
    "ax[0][0].set_xticks([])\n",
    "ax[0][0].set_yticks([])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1,10, figsize=(20, 20),squeeze=False)\n",
    "\n",
    "#######################\n",
    "\n",
    "# YOUR CODE HERE TO LOAD YOUR TEMPLATES\n",
    "# def preprocess_image(image):\n",
    "#     # gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "#     blurred = cv.GaussianBlur(image, (3, 3), 0)\n",
    "#     edges = cv.Canny(blurred, 5, 40)\n",
    "#     return edges\n",
    "# edges_templates = [preprocess_image(t) for t in templates]\n",
    "templates = \n",
    "for i, template in enumerate(templates):\n",
    "    display_image(template, axes=ax[0][i])\n",
    "    ax[0][i].set_title(\"Templates {:d}\".format(i+1))\n",
    "    ax[0][i].set_xticks([])\n",
    "    ax[0][i].set_yticks([])\n",
    "#######################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QiQMlIUGpCr7"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "\\\n",
    "def preprocess_image(image):\n",
    "    # gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    blurred = cv.GaussianBlur(image, (5, 5), 0)\n",
    "    edges = cv.Canny(blurred, 50, 150)\n",
    "    return edges\n",
    "\n",
    "hidden_edges = preprocess_image(hidden_gray)\n",
    "\n",
    "scaled_templates = [cv.resize(t, (int(t.shape[1] * 1.1), int(t.shape[0] * 1.1))) for t in templates]\n",
    "\n",
    "result_img, found_objects = find_objects(hidden_gray, hidden_rgb, scaled_templates)\n",
    "\n",
    "# Display the image with detected objects\n",
    "fig, ax = plt.subplots(1,1, figsize=(20, 20),squeeze=False)\n",
    "\n",
    "display_image(result_img, axes=ax[0][0])\n",
    "ax[0][0].set_title(\"Input\")\n",
    "ax[0][0].set_xticks([])\n",
    "ax[0][0].set_yticks([])\n",
    "\n",
    "plt.show()\n",
    "fig, ax = plt.subplots(1,10, figsize=(20, 20),squeeze=False)\n",
    "\n",
    "for i, template in enumerate(templates):\n",
    "    display_image(template, axes=ax[0][i])\n",
    "    ax[0][i].set_title(\"Templates {:d}\".format(i+1))\n",
    "    ax[0][i].set_xticks([])\n",
    "    ax[0][i].set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yDk_NytpCr7"
   },
   "source": [
    "## 2.2 Person Detection\n",
    "(*25 points*)\n",
    "\n",
    "In this section, we will return to the HOG features from the last Chapter. As we said before, HOG was proposed as a useful feature for human detection. If you reach this point, you may have noticed that Template matching may not be the best option for this. Imagine how difficult it would be to create a template for any human-shaped structure that you would like to detect as a human in a scene. Instead of that, you will train a Linear Classifier from scratch.\n",
    "\n",
    "In this section, you will use OpenCV's implementation to extract the HOG's features of the INRIA's Persons dataset to train an SVM Linear classifier (https://en.wikipedia.org/wiki/Support_vector_machine). For this, we will train the SVM classifier using the Scikit-learn Machine Learning library (http://scikit-learn.org).\n",
    "\n",
    "### 2.2.1 Dataset \n",
    "\n",
    " This dataset was collected as part of the research work on detection of upright people in images and video. The research is described in detail in the CVPR 2005 paper _Histograms of Oriented Gradients for Human Detection_. The full dataset is about ~1 GB and contains several thousands of pedestrian images. \n",
    " \n",
    "For your convenience, the dataset is already separated into two sets: \n",
    "* \"**_Positives_**\" which are all the images containing at least one person. \n",
    "* \"**_Negatives_**\" any kind of non-human shaped objects images.\n",
    "\n",
    "In addition, the data is already separated in a **training** and **testing** set.\n",
    "\n",
    "\n",
    "You can download the dataset on Moodle and put the `.tar` file into the `data` folder. To uncompress the data, you can either use the function below or directly do it in your file explorer, it is up to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oL8swTCEpCr8"
   },
   "outputs": [],
   "source": [
    "def maybe_extract(filename, force=False):\n",
    "    \"\"\"\n",
    "    Uncompress a given *.tar file\n",
    "    :param filename: File to be uncompressed\n",
    "    \"\"\"\n",
    "    # remove .tar.gz\n",
    "    root = os.path.splitext(os.path.splitext(filename)[0])[0] \n",
    "    if os.path.isdir(root) and not force:\n",
    "        # You may override by setting force=True.\n",
    "        print('%s dataset (seems to be) already present.\\nSkipping extraction of %s.' % (root, filename))\n",
    "    else:\n",
    "        print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "        tar = tarfile.open(filename)\n",
    "        tar.extractall(os.path.dirname(filename))\n",
    "        tar.close()\n",
    "    print(\"All setup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QYtS6weLpCr8"
   },
   "outputs": [],
   "source": [
    "# Optional\n",
    "maybe_extract(filename='../data/INRIAPerson.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rAkpBIQ4pCr9"
   },
   "source": [
    "### 2.2.2 Features Extraction\n",
    "\n",
    "Once you have the data, you will now process each image using the OpenCV HOGDescriptor implementation [[doc](https://docs.opencv.org/4.0.0/d5/d33/structcv_1_1HOGDescriptor.html)]. \n",
    "\n",
    "In the next cell, we define the parameters of the HoG descriptor and load the path to the images with the function `return_img_path()`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_img_path(sample_file, folder):\n",
    "    with open(sample_file, 'rt') as f:\n",
    "        img_path = []\n",
    "        for im_path in f:\n",
    "            if im_path.strip():\n",
    "                path = os.path.join(folder, im_path.strip())\n",
    "                img_path.append(path)\n",
    "    return img_path\n",
    "    \n",
    "\n",
    "# Data location\n",
    "#Positive folder:\n",
    "pos_im_path = \"../data/inria_train_pos.txt\"\n",
    "#Negative folder:\n",
    "neg_im_path = \"../data/inria_train_neg.txt\"\n",
    "    \n",
    "\n",
    "path_img_pos = return_img_path(pos_im_path, folder='../data/INRIAPerson')   \n",
    "path_img_neg = return_img_path(neg_im_path, folder='../data/INRIAPerson')   \n",
    "\n",
    "\n",
    "# Image descriptor parameters\n",
    "# ---------------------------\n",
    "# Window size: this specifies the size of the input image (remember to scale the input to this size!)\n",
    "win_size     = (64,128)\n",
    "# Size on pixels of each block (remember that a block contains a set of CELLS)\n",
    "block_size   = (16,16)\n",
    "# The separation between each block. If this value is less than the block size, \n",
    "# there will be overlapping blocks. \n",
    "block_stride = (8,8)\n",
    "# The size of each CELL. Each cell computes one histogram. The cells should FIT inside a block.\n",
    "cell_size = (4,4)\n",
    "# Number of bins for each histogram.\n",
    "nbins = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTIONS (/3)** \n",
    "\n",
    "In the next cell, implement the function `_extract_feature()` that takes as input a list of path to images, the HoG descriptor, the label of the images and the input size of the HoG descriptor. The function returns a list of HoG features for each image and a list of corresponding label. \n",
    "\n",
    "**WARNING**: The dataset may contain corrupted images. Be sure, inside your code, to check if the image was loaded properly. Otherwise, you will get either trash features or execution errors.\n",
    "\n",
    "\n",
    "Sometimes OpenCV can not load the image properly, that's why we're gonna use another package to load them. The command below shows how to achieve the same as `cv2.imread(...)` function.\n",
    "\n",
    "```python\n",
    "np.flip(np.asarray(imageio.imread('<image_path>', dtype=np.uint8))[..., :3], -1)\n",
    "```\n",
    "\n",
    "The `imread(...)` function load the image, it is then converted to numpy array with `np.asarray()`. Only the first 3 channels are selected with `[..., :3]` and finally it is converted from `RGB` to `BGR` with the `np.flip(..., -1)` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_feature(img_path, descriptor, label, hog_size):\n",
    "    \n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    ## YOUR CODE HERE\n",
    "    \n",
    "    ### Test if image loaded properly\n",
    "    for p in img_path:\n",
    "        image = np.flip(np.asarray(imageio.imread(p)).astype(np.uint8)[..., :3], -1)\n",
    "        resized = cv.resize(image, hog_size, interpolation = cv2.INTER_AREA)\n",
    "        ### Transfom to grayscale\n",
    "        image = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)\n",
    "        HOGs = descriptor.compute(image)\n",
    "        features.append(HOGs)\n",
    "        labels.append(label) \n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell loads the HoG features and labels for the positive and negatives images of the training set.\n",
    "\n",
    "**QUESTIONS (/2)**\n",
    "- The length of each individual feature vector should be of 16800. Why?\n",
    "- What should be the size of the _training_features_ list? Why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OpfVffhppCr-"
   },
   "outputs": [],
   "source": [
    "\n",
    "# hog is an instance taht contains the info and is able to compute the feature vector.\n",
    "hog = cv2.HOGDescriptor(win_size, block_size, block_stride, cell_size, nbins)\n",
    "\n",
    "\n",
    "# lists to save the features and labels.\n",
    "training_features = []\n",
    "labels = []\n",
    "\n",
    "# Positive features\n",
    "print (\"Calculating the descriptors for the positive samples and saving them\")\n",
    "pos_feat, pos_label = _extract_feature(path_img_pos, descriptor=hog, label=1, hog_size=win_size)   \n",
    "training_features.extend(pos_feat)\n",
    "labels.extend(pos_label)\n",
    "\n",
    "#Negative features\n",
    "print (\"Calculating the descriptors for the negative samples and saving them\")\n",
    "neg_feat, neg_label = _extract_feature(path_img_neg, descriptor=hog, label=0, hog_size=win_size)   \n",
    "training_features.extend(neg_feat)\n",
    "labels.extend(neg_label)\n",
    "\n",
    "# Summary\n",
    "print('#Samples: {}'.format(len(training_features)))\n",
    "print('Feature size: {}'.format(training_features[0].shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**\n",
    "- The length of each individual feature vector should be of 16800. Why?\n",
    "\n",
    "This is because of the size of the image, of the cells, the number of bins of the histograms and the block stride. We can compute it as follow:\n",
    "- Image size = (64, 128)\n",
    "- Block size = (16, 16)\n",
    "- Block stride = (8, 8)\n",
    "\n",
    "This means that the block can be slided **15** times on the horizontal axis (sliding a 16x16 block with steps of size 8 can be done 15 times on width of 128). The block can be slided **7** times on the vertical axis (sliding a 16x16 block with steps of size 8 can be done 7 times on height of 64). Each time we are sliding a block, we compute **16** HoG (one per cell in the block) whose size is **10**.\n",
    "\n",
    "Multiplying out the whole leads to the final dimension of the feature vector of the image: `dim` = `15` * `7` * `16` * `10` = `16800`\n",
    "\n",
    "- What should be the size of the _training_features_ list? Why?\n",
    "\n",
    "This list should have a length of 3634, since we have grouped all images together. There are 2416 positive images listed in the file `inria_train_pos.txt` and 1218 negative images listed in the file `inria_train_neg.txt`, so `2416`+`1218`=`3634`. Each of these sample has 16'800 dimensions as it is the feature vector of an image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipIoqZwdpCr_"
   },
   "source": [
    "### 2.2.3 Classifier\n",
    "\n",
    "Now that you have the HoG features, you'll train a Linear Support Vector Classifier [[doc](\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html)] using Scikit-learn. \n",
    "\n",
    "Please take the time to read the implementation details, but more importantly, the examples and theory provided in the documentation. \n",
    "\n",
    "**QUESTION (/5)** \n",
    "\n",
    "In the next cell, train a SVM classifier on the HoG features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eXTBqug0pCr_"
   },
   "outputs": [],
   "source": [
    "# Stack features\n",
    "train_features = np.asarray(training_features)\n",
    "train_labels = np.asarray(labels)\n",
    "\n",
    "# Train classifier\n",
    "# Check the dimentions first\n",
    "print('Feature matris dims: {}, labels dims: {}'.format(train_features.shape, \n",
    "                                                        train_labels.shape))\n",
    "\n",
    "#########################\n",
    "## YOUR CODE HERE\n",
    "# create a LINEAR classifier instance here (LinearSVC): \n",
    "\n",
    "model = LinearSVC(loss='hinge',max_iter=10000,C=0.01)\n",
    "# Train the classifier (LinearSVC.fit).\n",
    "print (\"Training a Linear SVM Classifier\")\n",
    "model.fit(training_features, labels)\n",
    "\n",
    "print('Done')\n",
    "\n",
    "\n",
    "#########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Justification of the parameters\n",
    "\n",
    "In this case we chose a `C = 1.0` which is the default parameter for LinearSVC for sklearn and we extended max_iter to `10'000` (the default value is `1'000`). This extension was made for the sake of the accuracy. Enabling more iterations leads to a longer computation time but also increases the precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1zErV1mpCsA"
   },
   "source": [
    "### 2.2.4 Evaluation\n",
    "\n",
    "After training the classifier, you can use this Support Vector Machine to *classify* if an * HOG feature vector* (with strictly the same dimensions as your training data) comes from an image with a human-shaped form in it (prediction = 1), or not (prediction = 1).\n",
    "\n",
    "\n",
    "**QUESTIONS (/10)**\n",
    "In the next cell, for each image in the test folders: \n",
    "* Compute the HOG feature vector,\n",
    "* Predict/classify the vector as positive (1) or negative (0),\n",
    "* Compute the estimation error for the negative and positive images _separetely_,\n",
    "* Compute and report the F1-score [[doc](https://en.wikipedia.org/wiki/F1_score)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "axNGwl3KpCsA"
   },
   "outputs": [],
   "source": [
    "test_neg_path = \"../data/inria_test_neg.txt\"\n",
    "test_pos_path = \"../data/inria_test_pos.txt\"\n",
    "\n",
    "###################\n",
    "# YOUR CODE HERE \n",
    "print (\"Estimating the test data [Negative samples]\")\n",
    "# Extract features\n",
    "# Predict labels \n",
    "# Prediction Error \n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "path_img_neg = return_img_path(test_neg_path, folder='../data/INRIAPerson')   \n",
    "test_neg_feat, label_neg_test = _extract_feature(path_img_neg, hog, label=0, hog_size=win_size)\n",
    "\n",
    "test_features_n = np.asarray(test_neg_feat)\n",
    "test_labels_n = np.asarray(label_neg_test)\n",
    "\n",
    "prediction_neg = model.predict(test_features_n)\n",
    "tn = accuracy_score(prediction_neg, test_labels_n)          \n",
    "print(\"Success rate negative samples:\", tn)\n",
    "\n",
    "print (\"Estimating the test data [Positive samples]\")\n",
    "# Extract features\n",
    "# Predict labels \n",
    "# Prediction Error\n",
    "###################\n",
    "                               \n",
    "path_img_pos = return_img_path(test_pos_path, folder='../data/INRIAPerson')\n",
    "test_pos_feat, label_pos_test = _extract_feature(path_img_pos, hog, label=1, hog_size=win_size)\n",
    "                               \n",
    "test_features_p = np.asarray(test_pos_feat)\n",
    "test_labels_p = np.asarray(label_pos_test)\n",
    "\n",
    "prediction_pos = model.predict(test_features_p)\n",
    "tp = accuracy_score(prediction_pos, test_labels_p) \n",
    "print(\"Success rate positive samples:\", tp)\n",
    "\n",
    "### F1 score\n",
    "fn = 1-tn\n",
    "fp = 1-tp\n",
    "err = 2*tp/(2*tp+fn+fp)\n",
    "\n",
    "# Compute score\n",
    "print('Estimation Error: {:.3f}%'.format(err * 100.0))\n",
    "print('Success Rate: {:.3f}%'.format(err * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysRMhrt3pCsB"
   },
   "source": [
    "The basic classifier above can (should) achieve a success rate of 89% for the positive and less than 2% error for the negative images respectively. \n",
    "\n",
    "**QUESTION** (/5)\n",
    "Can you tweak the HOG parameters to improve a little bit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will try to implement the same method using a higher number of bins. The performance should increase since adding bins leads to more accurate prediction as the histograms will detect changes along more directions. Each bin of the histogram corresponds to the variation of the gradient amplitude along a certain range of the phase. Increasing the number of bins implies that those variations of the gradient amplitude in the image will be noticed in a higher number of directions since the phase range will be smaller which increases the accuracy. Thus, the detection performance will increase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XuFuFvbQpCsB"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "\n",
    "### Tune HOG values\n",
    "win_size     = (64,128)\n",
    "block_size   = (16,16)\n",
    "block_stride = (8,8)\n",
    "cell_size = (4,4)\n",
    "nbins = 16  ### Take larger number of bins\n",
    "new_hog = cv2.HOGDescriptor(win_size, block_size, block_stride, cell_size, nbins)\n",
    "\n",
    "### Train feature extraction\n",
    "training_features = []\n",
    "labels = []\n",
    "print (\"Calculating the descriptors for the positive samples and saving them\")\n",
    "pos_feat, pos_label = _extract_feature(path_img_pos, descriptor=new_hog, label=1, hog_size=win_size)   \n",
    "training_features.extend(pos_feat)\n",
    "labels.extend(pos_label)\n",
    "\n",
    "#Negative features\n",
    "print (\"Calculating the descriptors for the negative samples and saving them\")\n",
    "neg_feat, neg_label = _extract_feature(path_img_neg, descriptor=new_hog, label=0, hog_size=win_size)   \n",
    "training_features.extend(neg_feat)\n",
    "labels.extend(neg_label)\n",
    "\n",
    "new_model = LinearSVC(loss='hinge',max_iter=10000,C=0.01)\n",
    "print (\"Training a Linear SVM Classifier\")\n",
    "new_model.fit(training_features, labels)\n",
    "\n",
    "### Test features extraction\n",
    "\n",
    "path_img_neg = return_img_path(test_neg_path, folder='../data/INRIAPerson')   \n",
    "test_neg_feat, label_neg_test = _extract_feature(path_img_neg, new_hog, label=0, hog_size=win_size)\n",
    "\n",
    "test_features_n = np.asarray(test_neg_feat)\n",
    "test_labels_n = np.asarray(label_neg_test)\n",
    "\n",
    "print (\"Estimating the test data [Negative samples]\")\n",
    "prediction_neg = new_model.predict(test_features_n)\n",
    "tn = accuracy_score(prediction_neg, test_labels_n)          \n",
    "print(\"Success rate negative samples:\", tn)\n",
    "\n",
    "print (\"Estimating the test data [Positive samples]\")                               \n",
    "test_pos_feat, label_pos_test = _extract_feature(path_img_pos, new_hog, label=1, hog_size=win_size)\n",
    "                               \n",
    "test_features_p = np.asarray(test_pos_feat)\n",
    "test_labels_p = np.asarray(label_pos_test)\n",
    "\n",
    "prediction_pos = new_model.predict(test_features_p)\n",
    "tp = accuracy_score(prediction_pos, test_labels_p) \n",
    "print(\"Success rate positive samples:\", tp)\n",
    "\n",
    "### F1 score\n",
    "fn = 1-tn\n",
    "fp = 1-tp\n",
    "err = 2*tp/(2*tp+fn+fp)\n",
    "\n",
    "# Compute score\n",
    "print('Estimation Error: {:.3f}%'.format(err * 100.0))\n",
    "print('Success Rate: {:.3f}%'.format(err * 100.0))\n",
    "\n",
    "### Try to change the phase : 360° -> 180°\n",
    "### cf toward datascience\n",
    "### preprocess the data\n",
    "### previous notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now observe that the performance is much better using a larger number of bins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OENlhyn0pCsC"
   },
   "source": [
    "Finally, from the description above, what we created is no more than a *classifier* for only 2 classes (binary classifier): human(human-shaped) or not. In order to create a functional Person detector for arbitrary images or video sequences, some engineering techniques (heuristics) need to be implemented. \n",
    "\n",
    "Pretty much as in template matching, in order to find a person in an arbitrary image you will need to: \n",
    "\n",
    "* Slide your classifier over the full area of the image.\n",
    "* Detect possible matchings. \n",
    "* Report them as positive or negatives\n",
    "* And optionally, repeat the procedure above in different scales, to assure multiscale detection!.\n",
    "\n",
    "The procedure is nicely depicted in the image below for face detection.\n",
    "\n",
    "<img src=\"https://www.pyimagesearch.com/wp-content/uploads/2015/03/sliding-window-animated-adrian.gif\"> \n",
    "</img>\n",
    "_Image taken from: https://www.pyimagesearch.com_\n",
    "\n",
    "For now, we will leave those implementation details for later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ErxjHbL3pCsC"
   },
   "source": [
    "## 2.3 Face Recognition\n",
    "(*35 points*)\n",
    "\n",
    "In this section you will implement a face regognition algorithm using two approaches: `Eigen Faces` and `Fisher Faces` . These methods are both based dimensional reduction technics listed below:\n",
    "\n",
    "- Principal Component Analysis ([*PCA*](http://www.utdallas.edu/~herve/abdi-awPCA2010.pdf))\n",
    "- Linear Discriminant Analysis (*LDA*)\n",
    "\n",
    "In general, in order to train a recognizer, several steps are needed and can be grouped as follow:\n",
    "\n",
    "- Data preparation\n",
    "- Recognizer training\n",
    "- System validation\n",
    "\n",
    "During validation, the system is tested with **unseen** data, i.e. the data hadn't used during the training phase, which ensures a fair performance assessment without biais. However this does not garantie that the system will *generalize* well to other dataset.\n",
    "\n",
    "### 2.3. 1 Data Preparation\n",
    "\n",
    "Data preparation covers various aspect of pre-processing step for training a system. At first, the images need to be splitted into two disctinct subsets thant will be used for `training` and `testing`. In our experiment the dataset used is the *Yale dataset version B*, it includes a total of 38 different identity (*i.e. subject*) each having 20 images undergoing different illumination condition for a total of 760 samples. We split the data into `training` and `testing` sets by selecting randomly samples from each subjects and placed into the corresponding subset. Special care needs to be taken in order to avoid having the same example multiple time.\n",
    "\n",
    "The first task is to gather the labels and the images that will be used to train the system. One solution is to store these information into a dictionary where the identity is the key and the pathes to the images for this subject are the values. \n",
    "\n",
    "**QUESTION** (/2)\n",
    "\n",
    "In the next cell, implement the function `load_dataset()` that takes as input the path to the dataset, and returns a dictionary in which the keys are the person ID and the values are the pathes to the corresponding images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qhBUaXsqpCsD"
   },
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    \"\"\"\n",
    "    Scan for images in a given `path` and extract the label as well\n",
    "\n",
    "    :param path:    Path where YaleB dataset is stored\n",
    "    :return:        Dictionary storing the ID and a list of images for this ID\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    for i in range(1, 40):\n",
    "        if i < 10:\n",
    "            path = os.path.join('..', 'data', 'yaleB', 'yaleB0'+str(i))\n",
    "            data[i] = path\n",
    "        elif i==14: ### 14 is not present in the folders\n",
    "            pass\n",
    "        else:\n",
    "            path = os.path.join('..', 'data', 'yaleB', 'yaleB'+str(i))\n",
    "            data[i] = path\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hFo32kCKpCsE"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = load_dataset(os.path.join('..', 'data', 'yaleB'))\n",
    "assert(len(data) == 38)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dui2K3uIpCsE"
   },
   "source": [
    "When all the images and labels have been gathered, the next step is to split into two subsets, the train set and test set. The training set will be composed of $75\\%$ of the images of each subject and the remaining $25\\%$ will be used as test set.\n",
    "\n",
    "Once again the two subsets information will be stored into two separates disctionary similar to what has been done earlier.\n",
    "\n",
    "\n",
    "**QUESTION** (/3)\n",
    "\n",
    "In the next cell, implement to the function `split_dataset()` that takes as input the dictionary of data and the ratio between training and testing sets, and returns two dictionaries, one with the training data and one with the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JdzA2MagpCsF"
   },
   "outputs": [],
   "source": [
    "def split_dataset(data, ratio):\n",
    "    \"\"\"\n",
    "    Splipt randomly a dataset into two subset. The ratio provide the distribution for each subset\n",
    "\n",
    "    :param data:    Overall dataset\n",
    "    :param ratio:   Split ratio\n",
    "    :return:        Two dictionaries, train/test\n",
    "    \"\"\"\n",
    "    train = {}\n",
    "    test = {}\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    import random\n",
    "    \n",
    "    for key in data.keys():\n",
    "        \n",
    "        ### Remove the Icon_ file from the pictures\n",
    "        files = [file for file in os.listdir(data[key]) if file.endswith(\".pgm\")]\n",
    "        \n",
    "        ### Shuffle the pictures before splitting\n",
    "        random.shuffle(files)\n",
    "        \n",
    "        for i, pict in enumerate(files):\n",
    "            if i == 0:\n",
    "                train[key] = []\n",
    "                train[key].append(os.path.join(data[key], pict))\n",
    "            elif i < int(ratio*len(data[key])):\n",
    "                train[key].append(os.path.join(data[key], pict))\n",
    "            elif key in test.keys():\n",
    "                test[key].append(os.path.join(data[key], pict))\n",
    "            else:\n",
    "                test[key] = []\n",
    "                test[key].append(os.path.join(data[key], pict))\n",
    "        \n",
    "                \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell splits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yi4Jxc40pCsF",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train, test = split_dataset(data, 0.75)\n",
    "assert(len(train) == 38)\n",
    "assert(len(test) == 38)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30P9zUEApCsF"
   },
   "source": [
    "Now with these two subsets we can load the images and extract features from them. In this case the pixel intensity of grayscale images will be used as feature, therefore for an image $I \\in \\mathbb{R}^{ w \\times h}$ the feature vector will have a size of $wh$. This value can be quite large very easily, therefore all images will be downsampled by a factor of $2$.\n",
    "\n",
    "All the training samples will be concatenated into a single matrix where each row is an image (*i.e. flattened*) with dimensions $N \\times K$ where $N$ is the number of samples and $K$ is the dimension of a single image, $K = \\frac{wh}{4}$. Also, make sure the features are float32 normalized between 0 and 1. \n",
    "\n",
    "The corresponding labels will also be concatenated into a single vector of dimension $N \\times 1$.\n",
    "\n",
    "\n",
    "**QUESTION** (/5)\n",
    "\n",
    "In the next cell, implementto the function `load_images()` that takes as input a dictionary of data, and returns the feature matrix and the label vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KLSP_DeCpCsG"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing as pre\n",
    "\n",
    "def load_images(subset):\n",
    "    \"\"\"\n",
    "    Load images into one single matrix where each row is one single image (flattened). The final dimensions is [N x K]\n",
    "    where N is the numper of samples available and K is the number of pixel in one image. The original image is first\n",
    "    downspample by a factor of 2\n",
    "\n",
    "    Labels are also exported into a single vector of dimensions [N x 1]\n",
    "\n",
    "    :param subset: Dictionary storing labels/images\n",
    "    :return:       Data matrix and label vector\n",
    "    \"\"\"\n",
    "    \n",
    "    data = None\n",
    "    label = None\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    label = []\n",
    "    data = []\n",
    "    for i in range(1, len(subset)+2):\n",
    "        \n",
    "        ### No folder yaleB14\n",
    "        if i != 14:\n",
    "            for file in subset[i]: \n",
    "                \n",
    "                ### Ensure take a .pgm file\n",
    "                if file.endswith(\".pgm\"):\n",
    "                    image = cv2.cvtColor(cv.imread(file), cv2.COLOR_BGR2GRAY).astype(np.float32)\n",
    "                    dim = image.shape\n",
    "                    new_im = cv2.resize(image, (int(dim[1]/2), int(dim[0]/2)))\n",
    "                    x = new_im.flatten()\n",
    "                    x_norm = (x-np.min(x))/(np.max(x)-np.min(x))\n",
    "                    data.append(x)\n",
    "                    label.append(i)\n",
    "                    \n",
    "    data, label = np.array(data), np.array(label)    \n",
    "\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell loads the training data and the labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8sDGg8oTpCsG"
   },
   "outputs": [],
   "source": [
    "# Load training images into memery\n",
    "train_img, train_label = load_images(train)\n",
    "# Sanity check\n",
    "assert(train_img.shape[0] == train_label.size)\n",
    "# Output number of samples\n",
    "print(\"There is a total of {} samples for the training set\".format(train_label.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zS5YXWUopCsH"
   },
   "source": [
    "\n",
    "### 2.3.2 Eigenfaces\n",
    "\n",
    "To perform recognition, all pixel's intensities are used as feature vector. The dimension of theses descriptors will be large, therefore a *clever* representation of the data, called subspace, is needed. \n",
    "\n",
    "This subspace is computed using *Principal Component Analysis* method in order to extract meaningfull information and reduce the dimension of the problem. The *PCA* approach is completely unsupervised and extract the directions, or *basis*, where the variation in the data are the largest inside the feature space. \n",
    "\n",
    "Since we are interested in the variation in the data, the first step is to remove the commmon information present in all samples by subtracting the **average face**. The average is computed using all training samples $I_i$ as follow:\n",
    "\n",
    "$$\n",
    "\\bar{\\boldsymbol{I}} = \\frac{1}{N_t} \\sum_{i=0}^{N_t} \\boldsymbol{I}_i\n",
    "$$\n",
    "\n",
    "where $N_t$ is the total number of training samples and $I_i$ is a specific training sample. Then each samples $I_i$ are normalized as follow:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\phi_i} = \\boldsymbol{I}_i - \\bar{\\boldsymbol{I}}\n",
    "$$\n",
    "\n",
    "With all samples normalized, we need to find a set of orthognonal basis which best explain the distribution of our data. To do so we compute the eigendecomposition of the covariance matrix of the normalized samples.\n",
    "\n",
    "$$\n",
    "\\boldsymbol{C} = \\frac{1}{N_t - 1} \\sum_{i}^{N_t} \\boldsymbol{\\phi}_i\\boldsymbol{\\phi}_i^{\\top} = \\frac{1}{N_t - 1} \\boldsymbol{\\Phi\\Phi}^{\\top}, \\quad \\text{where } \\Phi = \\left[\\boldsymbol{\\phi}_0, \\dots, \\boldsymbol{\\phi}_{N} \\right]\n",
    "$$\n",
    "\n",
    "Find the eigenvectors $u_k$ and the eigenvalues $\\lambda_k$. \n",
    "\n",
    "So far the dimensions of the problem have not been reduced. Moreover the size of the covariance matrix will be $K \\times K$ with $K = \\frac{wh}{4}$. Therefore we will find $K$ eigenvectors representing the variation in our data. To reduce the dimension we will select only the eigenvectors that contribute the most to the variation and dropping the one with little influence. Doing so will reduce the dimension of the subspace to $K \\times K_m$.\n",
    "\n",
    "The question is how to properly determine this $K_m$ value. It can be done by using the eigenvalues computed earlier. These values are representing the energy each vector contribute for. Therefore it is possible to dertmine the number of basis to select in order to retain a certain amount of energy.\n",
    "\n",
    "$$\n",
    "\\frac{\\sum_{k=0}^{K_m}\\lambda_k}{\\sum_{i=0}^{K}\\lambda_i} < \\Theta\n",
    "$$\n",
    "\n",
    "Where $\\Theta$ represents the amount of energy to retain, which usually is around $95\\%$ but can vary depending on the application. \n",
    "Finally the subspace is defined as:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{U} = \\left[\\boldsymbol{u}_0, \\dots, \\boldsymbol{u}_{K_m}\\right], \\quad \\boldsymbol{U} \\in \\mathbb{R}^{K \\times K_m}\n",
    "$$\n",
    "\n",
    "In practice, the PCA decomposition is computed using `sklearn.decomposition.PCA`, more information can be found [here](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html).\n",
    "\n",
    "\n",
    "**QUESTION** (/3)\n",
    "\n",
    "In the next cell, compute the face subspace on the training data by retaining $95\\%$ of the variance present in the training data.\n",
    "\n",
    "Once the subspace is computed, display the first $8$ modes or *eigenfaces*. \n",
    "Comment on what you see. What do you thing are the limitations of such approach ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ET1Z8NSdpCsI",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "train_centered = train_img-train_img.mean(axis=0)\n",
    "#plot_gallery(\"Faces from dataset\", train_centered[:6])\n",
    "pca = PCA()\n",
    "pca.fit(train_centered)\n",
    "#pca_estimator = PCA(n_components=8, svd_solver=\"randomized\", whiten=True)\n",
    "#pca_estimator.fit(train_centered)\n",
    "\n",
    "m, summ = 0, 0\n",
    "\n",
    "for i, var in enumerate(pca.explained_variance_ratio_):\n",
    "    summ += var\n",
    "    if summ>0.95:\n",
    "        m = i\n",
    "        break\n",
    "        \n",
    "print(\"The total number of components to retain 95% of the variance of the data is:\", m)\n",
    "eigenfaces = pca.components_[:m]    \n",
    "print(eigenfaces.shape)\n",
    "dim = cv.imread(train[1][0]).shape[:2]\n",
    "\n",
    "\n",
    "for i in range(8):\n",
    "    plt.subplot(2,4,i+1)\n",
    "    plt.title(\"EigenFace #\"+str(i))\n",
    "    plt.imshow(eigenfaces[i, :].reshape(int(dim[0]/2), int(dim[1]/2)), cmap='gray') ### Eigenfaces instead of pc I think\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PCA from scratch \n",
    "\n",
    "X = train_img\n",
    "mean = np.matmul(np.ones(X.shape[0]).reshape(X.shape[0],1),X.mean(0).reshape(1,X.shape[1]))\n",
    "X = X - mean\n",
    "\n",
    "S = np.matmul(X, X.transpose())*(1/X.shape[0])\n",
    "w, v = np.linalg.eig(S)\n",
    "\n",
    "total_var = np.cumsum(w)/w.sum()\n",
    "num1 = total_var.shape[0] - (total_var > 0.95).sum()\n",
    "num2 = total_var.shape[0] - (total_var > 0.99).sum()\n",
    "red1 = np.round(100*(1-num1/total_var.shape[0]),2)\n",
    "red2 = np.round(100*(1-num2/total_var.shape[0]),2)\n",
    "print('To represent 95% of the total variation, we need ' + str(num1) + ' eigenvectors out of ' + str(total_var.shape[0]) + ' ( ' + str(red1) + '% reduction )' )\n",
    "print('To represent 99% of the total variation, we need ' + str(num2) + ' eigenvectors out of ' + str(total_var.shape[0]) + ' ( ' + str(red2) + '% reduction )' )\n",
    "\n",
    "#Get the principal components and plot\n",
    "plt.plot(w/w.sum())\n",
    "plt.title('Individual eigenvalue contribution')\n",
    "plt.ylabel('Percent contribution to variance')\n",
    "plt.xlabel('Eigenvalue number')\n",
    "plt.show()\n",
    "\n",
    "#Calculate the total percentage of variance explained\n",
    "total_var = np.cumsum(w)/w.sum()\n",
    "plt.figure()\n",
    "plt.plot(total_var)\n",
    "plt.title('Cumulative eigenvalue contribution')\n",
    "plt.ylabel('Percent contribution to variance')\n",
    "plt.xlabel('Eigenvalue number')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s, v = np.linalg.svd(X)\n",
    "pc = v.copy()\n",
    "for i in range(8):\n",
    "    plt.subplot(2,4,i+1)\n",
    "    plt.title(\"EigenFace #\"+str(i))\n",
    "    plt.imshow(pc[i, :].reshape(int(dim[0]/2), int(dim[1]/2)), cmap='gray') ### Eigenfaces instead of pc I think\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**\n",
    "\n",
    "First, we notice that both methods using sklearn and computed from scratch leads to the same results. We need the `48` first components to retain `95% of the variance` present in the data. We can also read it on graph 2 that represents the cumulated contribution of the eigenvalues.\n",
    "\n",
    "On the displayed images, we can distinguish 8 faces with a different illumination on each picture. For instance `EigenFace #0` is illuminated from the right while `EigenFace #1` is illuminated from the left and so forth. This variation in the illumination helps to distinguish the different features of the faces. For example, on `EigenFace #0` we can barely see the eyebrows on the face while they are easily observable on `EigenFace #3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCpE4iHdpCsK"
   },
   "source": [
    "In classification problem, it is always interesting to visualize the feature subspace to evaluate if they discriminate properly the different subjects. Having proper plots of an $N$ dimensional space is not feasable, and, therefore, we use only a few components (*i.e. 2 or 3*) of our projected samples. \n",
    "\n",
    "\n",
    "We want a subspace that is able to separate and cluster properly each subject to avoid miss recognition. The code snippet below shows the $3^{rd}$ and $4^{th}$ components on a $2D$ plane.\n",
    "\n",
    "**QUESTION** (/2)\n",
    "\n",
    "Do we have clean inter-subject separation ? Comment the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YPrEpu-OpCsK"
   },
   "outputs": [],
   "source": [
    "# Poject data onto subspace\n",
    "proj_train = pca.transform(train_img)\n",
    "\n",
    "# Visualize \n",
    "# Colors for distinct individuals\n",
    "colors = LabelEncoder().fit_transform(train_label.ravel())\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.scatter(proj_train[:, 2], proj_train[:, 3], c=colors)\n",
    "plt.xlabel('PC2')\n",
    "plt.ylabel('PC3')\n",
    "plt.title('Trainset clusters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**\n",
    "\n",
    "On the plot `Trainset clusters`, we can observe that the different colour points are mixed and not clearly seperable. Even if they each colour tends to be concentrated within the same region, there are other colours that are grouped almost at the same place. That means that we cannot find a clean inter-subject seperation at that point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9NswXgKpCsL"
   },
   "source": [
    "In this new subspace, each training sample has a representation, given by its projection into the eigen subspace (*i.e. eigenface*) as follow:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\omega}_i = \\boldsymbol{U}^\\top \\boldsymbol{\\phi}_i\n",
    "$$\n",
    "\n",
    "In the training set, there are multiple samples avaible for each subject. Their projection won't be exactly the same, and, therefore, we need a more generic representation of each person. One option is to average all representation of the specific person:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\Omega}_k = \\frac{1}{N_k} \\sum_{p} \\boldsymbol{\\omega}_p\n",
    "$$\n",
    "\n",
    "where $N_k$ is the number of samples for subject $k$ and $\\boldsymbol{\\omega}_p$ represents the projected samples of subject $k$. In our case, $k$ goes from $0$ to $37$.\n",
    "\n",
    "**QUESTION** (/3) \n",
    "\n",
    "In the next cell, implement the function `compute_centroids()` that takes as input the feature matrix, the transformation to be applied to the feature matrix and the corresponding label, and returns each subject's centroid, and the corresponding labels.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C3qO_ESNpCsM"
   },
   "outputs": [],
   "source": [
    "def compute_centroids(data, trsfrm, label):\n",
    "    \"\"\"\n",
    "    Given a list of training samples, compute the centroids of each uniques labels\n",
    "\n",
    "    :param data:    Matrix with all feature vectors stored as row\n",
    "    :param trsfrm:  Embeddings to use (object with `transform(X)` method available such as PCA/LDA from sklearn)\n",
    "    :param label:   List of corresponding labels\n",
    "    :return:        Centroids, unique labels\n",
    "    \"\"\"\n",
    "\n",
    "    centroids = None\n",
    "    unique_lbl = None\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    ### Try U = pca.component_[:m]\n",
    "    data_centered = data-data.mean(axis=0)\n",
    "    trsfrm.transform(data) # U = \n",
    "    U = trsfrm.components_[:m]\n",
    "    centroids = np.array(np.split(U@data_centered.T, 38, axis=1)) # not .T and axis =0\n",
    "    print(centroids.shape)\n",
    "    centroids = np.mean(centroids, axis=2) # axis = 2\n",
    "    \n",
    "    unique_lbl = np.unique(label)\n",
    "    print(centroids.shape)\n",
    "    \n",
    "    return centroids, unique_lbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell calculates the centroid of each subsject. \n",
    "\n",
    "**QUESTION** (/1)\n",
    "\n",
    "What is the number of centroid? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4N0nKB4NpCsM"
   },
   "outputs": [],
   "source": [
    "# Define centroids\n",
    "train_centroids, train_centroid_label = compute_centroids(train_img, pca, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of centroids is :\", train_centroids.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should have `38 centroids` since this is the number of subjects. As we computed the mean of all the projections of the training samples for each subject, this leads to 38 centroids and this is in agreement with the result printed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dkKVRn3spCsN"
   },
   "source": [
    "\n",
    "So far we've learned a face representation and computed the descriptors for the training samples. Ultimately, we want to use them to recognize face. The first step is to bring the *new* sample into our face *subspace*, similarly to the training samples:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\omega}_n = \\boldsymbol{U}^{\\top}(\\boldsymbol{I}_n - \\bar{\\boldsymbol{I}})\n",
    "$$\n",
    "\n",
    "where $U$ is the face subspace, $\\bar{\\boldsymbol{I}}$ is the average face learned on the training data, and $\\boldsymbol{I}_n$ is the new sample to recognize.\n",
    "\n",
    "In this subspace, we can measure the **similarity** (*distance*) between a testing sample and the centroids $\\boldsymbol{\\Omega}_k$ computed before. The predicted label will be the one corresponding the the closest centroid. \n",
    "\n",
    "$$ \n",
    "min \\left|\\left| \\boldsymbol{\\omega}_n - \\boldsymbol{\\Omega}_k \\right|\\right| \\quad \\forall k \\in \\{Train\\}\n",
    "$$\n",
    "\n",
    "\n",
    "**QUESTION** (/2)\n",
    "\n",
    "In the next cell, implement the function `recognize` that takes as input the transformation, the centroids, the centroid labels and the new samples, and returns the predicted labels of the new samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zoirPAY0pCsO"
   },
   "outputs": [],
   "source": [
    "\n",
    "def recognize(trsfrm, centroids, centroids_label, samples):\n",
    "    \"\"\"\n",
    "    Perform object recognition on a given list of ``amples\n",
    "\n",
    "    :param trsfrm:          Embeddings to use (object with `transform(X)` method available such as PCA/LDA from sklearn)\n",
    "    :param centroids:       List of centroids learned in training phase\n",
    "    :param centroids_label: Label corresponding to the centroids\n",
    "    :param samples:         List of samples to recognize\n",
    "    :return:                Predicted labels\n",
    "    \"\"\"\n",
    "\n",
    "    pred = None \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    data_centered = samples-samples.mean(axis=0)\n",
    "    trsfrm.transform(samples).T # U = \n",
    "    U = trsfrm.components_[:m]\n",
    "    #omega_n = np.array(np.split(U@train_centered, 38, axis=0))\n",
    "    \n",
    "    X = samples\n",
    "    #X = X-X.mean(axis=0)\n",
    "    mean = np.matmul(np.ones(X.shape[0]).reshape(X.shape[0],1),X.mean(0).reshape(1,X.shape[1]))\n",
    "    X = X - mean\n",
    "\n",
    "    omegas = U@X.T\n",
    "    pred = []\n",
    "    print(omegas.shape, X.shape)\n",
    "\n",
    "    for om in omegas.T: # no .T\n",
    "        dist = []\n",
    "        for c in centroids:\n",
    "            dist.append(np.linalg.norm(om-c))\n",
    "        i = np.argmin(np.array(dist))\n",
    "        \"\"\"\n",
    "        #dist = euclidean_distances(centroids.T, (om.reshape(-1, 1)@np.ones((1, centroids.shape[1]))))\n",
    "        dist = euclidean_distances(centroids, om.reshape(-1, 1).T)\n",
    "        print(dist)\n",
    "        print(dist.shape, centroids.shape, om.reshape(-1, 1).T.shape)\n",
    "        i = np.argmin(dist, axis=1)\n",
    "        \"\"\"\n",
    "        pred.append(centroids_label[i])\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3mIO2EB1pCsP"
   },
   "source": [
    "The next cell validates the implementation with the whole system on the training set. The expected recognition accuracy should by close to 100% depending on the task difficulty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9J6P5gB5pCsP"
   },
   "outputs": [],
   "source": [
    "# Recognize training set\n",
    "pca_train_pred = recognize(pca, train_centroids, train_centroid_label, train_img)\n",
    "\n",
    "# Compute performance\n",
    "n_err = np.count_nonzero(np.where(pca_train_pred != train_label))\n",
    "train_acc = 1.0 - n_err / train_label.size\n",
    "print(\"The recognition accuracy on the trainig set is {:.2f}\".format(train_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOvsw6yIpCsP"
   },
   "source": [
    "**QUESTIONS** (/2)\n",
    "\n",
    "In the nex cell, estimate the recognition accuracy on the testing set. Comment the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h9GrcOTnpCsP"
   },
   "outputs": [],
   "source": [
    "test_acc = 0\n",
    "\n",
    "# YOUR CODE HERE\n",
    "test_img, test_label = load_images(test)\n",
    "test_centered = test_img-test_img.mean(axis=0)\n",
    "pca_test_pred = recognize(pca, train_centroids, train_centroid_label, test_img)\n",
    "n_err = np.count_nonzero(np.where(pca_test_pred != test_label))\n",
    "test_acc = 1.0 - n_err / test_label.size\n",
    "\n",
    "print(\"The recognition accuracy on the testing set is {:.2f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**\n",
    "\n",
    "We can observe that the recognition accuracy on the `testing set` is **lower** than on the `training one`, but remains a good performance. This is what we should expect as we try to recognize new images that may be different from the training ones that were used to compute the centroids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvItBPfhpCsQ"
   },
   "source": [
    "Measuring accuracy of the system is a good indicator of the overall performance but does not indicate where the system is performing poorly. This can be quantified using **Confusion Matrix**. It describes the performance of classification model and shows how the system is confused for each samples in the training set.\n",
    "\n",
    "Such representation can be computed using `sklearn.metrics.confusion_matrix`, details are provided [here](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html).\n",
    "\n",
    "**QUESTION** (/2)\n",
    "\n",
    "In the next cell, calculate and visualize the confusion matrix using the function `plot_confusion_matrix()` of the `utils.py` file. Comment the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, \n",
    "                          classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          axes=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \n",
    "    :param cm: Confusion matrix\n",
    "    :param classes: Classes name\n",
    "    :param normalize: Indicate if the confusion matrix need to be normalized\n",
    "    :param title: Plot's title\n",
    "    :param axes: Subplot on which to display the image\n",
    "    :param cmap: Colormap to use\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    # Show cm\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    #fmt = '.2f' if normalize else 'd'\n",
    "    #thresh = cm.max() / 2.\n",
    "    #for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    #    plt.text(j, i, format(cm[i, j], fmt),\n",
    "    #             horizontalalignment=\"center\",\n",
    "    #             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-erz-7w5pCsR"
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "# YOUR CODE HERE\n",
    "#########################\n",
    "\n",
    "cm = confusion_matrix(pca_test_pred, test_label)\n",
    "classes = np.unique(train_label)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(12, 7))\n",
    "plot_confusion_matrix(cm, classes, normalize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**\n",
    "\n",
    "It can be seen that this matrix is **not perfectly symmetric** and is **not perfectly diagonal** even though it is close to being so. The values on each row should add up to one and so whenever we have an non-diagonal element not equal to zero, the diagonal element on the same row will be less than one. Those non-zero non-diagonal elements represent the location where a missclassification occured. The `x axis` indicates the label that was assigned to a picture and the `y axis` the true label of the picture.\n",
    "\n",
    "The diagonal elements represent the successful classifications as the predicted labels match the true ones.\n",
    "\n",
    "If the test prediction accuracy were to be 100% all the diagonal elements should be set to one 1 and the rest to 0 as we will see in section `2.3.3 Fisherface`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ecG7gz0KpCsS"
   },
   "source": [
    "### 2.3.3 Fisherface\n",
    "\n",
    "The subspace computed before with *PCA* was looking at directions where the variation in the data is maximum without paying attention to the class each data point belongs to. Therefore this approach is unsupervised. The major drawback is that the class separability is not garantee to be optimum. \n",
    "\n",
    "The approach of *Linear Discriminant Analysis* is to find a subspace where the variation is large (*i.e. similar to PCA*) but also to maximize the inter-class separability by taking into account each sample's label. The figure below shows an example:\n",
    "\n",
    "<img src=\"../data/lda_example.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "Such subspace can be computed as follow:\n",
    "\n",
    "- Compute the scatter matrices (*intra-classes* / *inter-classes*)\n",
    "- Compute the eigenvectors / eigenvalues\n",
    "- Select the $K_m$ largest eigenvalues and their corresponding eigenvectors\n",
    "\n",
    "Given a set of samples $\\boldsymbol{I}_0, \\dots, \\boldsymbol{I}_N$ and their corresponding labels $y_0, \\dots, y_N$, the intra-class scatter matrix is computed as follow:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{S}_w = \\sum_{i=1}^N \\left(\\boldsymbol{I}_i - \\boldsymbol{\\mu}_{y_i}\\right) \\left(\\boldsymbol{I}_i - \\boldsymbol{\\mu}_{y_i}\\right)^{\\top}\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\mu}_{k}$ is the sample mean of the $k^{th}$ class.\n",
    "Then the inter-class scatter matrix is defined as:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{S}_b = \\sum_{k=1}^{m} n_k (\\boldsymbol{\\mu}_k - \\boldsymbol{\\mu})(\\boldsymbol{\\mu}_k - \\boldsymbol{\\mu})^{\\top}\n",
    "$$\n",
    "\n",
    "where $m$ is the number of classes, $\\boldsymbol{\\mu}$ is the overall sample average and $n_k$ is the number of samples in the $k^{th}$ class.\n",
    "\n",
    "Finally the subspace $\\boldsymbol{W}$ can be computed by solving the following generalizeed eigenvalue problem:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{S}_b \\boldsymbol{W} = \\lambda \\boldsymbol{S}_w \\boldsymbol{w}\n",
    "$$\n",
    "\n",
    "Finally at most $m-1$ generalized eigenvectors are useful to discriminate between $m$ classes.\n",
    "\n",
    "In practice, such decomposition can be computed using `sklearn.discriminant_analysis.LinearDiscriminantAnalysis`, more information available [here](http://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html).\n",
    "\n",
    "**QUESTION** (/2)\n",
    "\n",
    "In the next cell, compute the *LDA* subspace similar to what you have done before and display the first 8 basis (*i.e. Fisherface*).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HcyRrioxpCsS"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "lda = LDA()\n",
    "lda.fit(train_img, train_label)\n",
    "fisherfaces = lda.coef_\n",
    "\n",
    "for i in range(8):\n",
    "    plt.subplot(2,4,i+1)\n",
    "    plt.title(\"EigenFace #\"+str(i))\n",
    "    plt.imshow(fisherfaces[i, :].reshape(int(dim[0]/2), int(dim[1]/2)), cmap='gray') ### Eigenfaces instead of pc I think\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "up7pije1pCsT"
   },
   "source": [
    "**QUESTIONS** (/2)\n",
    "\n",
    "In the next cell, visualize the subspace created by the *LDA* decomposition, similarly to the PCA method. \n",
    "\n",
    "What do you see? Is it better than before ? Comment the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ls1qZI8PpCsU"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "proj_train = lda.transform(train_img)\n",
    "\n",
    "# Visualize \n",
    "# Colors for distinct individuals\n",
    "colors = LabelEncoder().fit_transform(train_label.ravel())\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.scatter(proj_train[:, 2], proj_train[:, 3], c=colors)\n",
    "plt.xlabel('Axis 2')\n",
    "plt.ylabel('Axis 1')\n",
    "plt.title('Trainset clusters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**\n",
    "\n",
    "In this case, the subspace is more sparsed in the sense that we can see clusters that are aggregated by colour.  About thirty clusters can be seen, so can deduce that each cluster corresponds to a subject. This result is better than what we had for PCA as we can find an inter-subject seperation more easily. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N07YMqrVpCsV"
   },
   "source": [
    "**QUESTION** (/2)\n",
    "\n",
    "In the next cell, compute the recognition accuracy on the *training*/*testing* set for the *LDA* recognizer. Comment the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3A6Zc4EVpCsV"
   },
   "outputs": [],
   "source": [
    "train_acc = 0\n",
    "test_acc = 0\n",
    "\n",
    "######################\n",
    "# YOUR CODE HERE\n",
    "train_acc = lda.score(train_img, train_label)\n",
    "test_acc = lda.score(test_img, test_label)\n",
    "######################\n",
    "\n",
    "print(\"The recognition accuracy on the trainig set is {:.2f}\".format(train_acc))\n",
    "\n",
    "print(\"The recognition accuracy on the testing set is {:.2f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyKGFsCwpCsW"
   },
   "source": [
    "**QUESTION** (/2)\n",
    "\n",
    "Compute the *Confusion Matrix* and comment on the result you have\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SXyeo7VHpCsW"
   },
   "outputs": [],
   "source": [
    "\n",
    "######################\n",
    "cm = confusion_matrix(lda.predict(test_img), test_label)\n",
    "classes = np.unique(train_label) # or np.arange(38)\n",
    "######################\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(12, 7))\n",
    "plot_confusion_matrix(cm, classes, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**\n",
    "\n",
    "In the case of LDA, the matrix is symmetric and seems to be a pure diagonal matrix. The diagonal elements are equal to one and the other components are 0. This behaviour is unlike the one of the confusion matrix for PCA. The latter was not exaclty symmetric and some non diagonal terms were set to values between 0.2, 0.4 and not all the diagonal terms were set to one.\n",
    "\n",
    "This difference between PCA and LDA is also observable in the training and testing accuracy which is 1.00 for LDA whil it is lower for PCA and explains this difference on the confusion matrix. There are no off-diagonal elements because the classification error is 0%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAEs6S28pCsX"
   },
   "source": [
    "**QUESTION** (/2) \n",
    "\n",
    "You have implemented / tested two approaches for face recognition which one works the best and why ? What's are the pro/cons of each method ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6m6JVzmpCsX"
   },
   "source": [
    "**YOUR ANSWER**\n",
    "\n",
    "LDA seems to be the best method since it achieves an accuracy of 100% for both training and testing images. As we could observe with the projection of the data on the subspace of LDA and PCA, LDA achieve a much better seperability of the clusters and thus has a better performance.\n",
    "\n",
    "However, LDA may also have a drawbacks. \n",
    "- While PCA is an unsupervised learning method, LDA takes the class lables into account. This means that this method cannot be used without labels\n",
    "\n",
    "- This method finds a trade off between two factors. It tries to find a subspace where the variation is large but also aims to maximize the inter-class separability by taking into account each sample's label. This \n",
    "\n",
    "PCA's drawback is its accuracy that is in general lower than LDA in the case where we have access to labels. However, the pro of this method is that it can be used without labels whil LDA not as it is unsupervised method."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "02_object_detection_and_recognition_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "3196968d684371006099b3d55edeef8ed90365227a30deaef86e5d4aa8519be0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
