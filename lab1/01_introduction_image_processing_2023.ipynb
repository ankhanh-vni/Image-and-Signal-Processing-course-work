{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a59cf561",
   "metadata": {},
   "source": [
    "**Students' name**\n",
    "\n",
    "Mathieu Ruch\n",
    "\n",
    "An Bui Duc Khanh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edb65de",
   "metadata": {},
   "source": [
    "This aim of this course is to review the evolution of image processing tools, from basics to deep learning algorithms. The semester is split into four labs :\n",
    "\n",
    "* **Lab 1** : Introduction to Image Processing\n",
    "* **Lab 2** : Object detection\n",
    "* **Lab 3** : Object tracking\n",
    "* **Lab 4** : Introduction to Deep Learning for image classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9952878",
   "metadata": {},
   "source": [
    "Let's start with the first chapter of this course!\n",
    "\n",
    "# Chapter 1 : Introduction to Image Processing\n",
    "\n",
    "(100 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99be909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87b5fd3",
   "metadata": {},
   "source": [
    "## 1.1 Introduction to Basic Image Processing Using OpenCV and NumPy\n",
    "\n",
    "(20 points)\n",
    "\n",
    "In this section we start with the basic image processing in Python.\n",
    "\n",
    "**Goals:**\n",
    "\n",
    "1. Read an image, access and modify pixel values\n",
    "2. Change color space\n",
    "3. Understand the concepts of histogram and histogram equalization\n",
    "\n",
    "\n",
    "### 1.1.1 Read an image, access and modify pixel values\n",
    "\n",
    "The code in the next section shows how you can load an image into the Jupyter notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ec27e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reading an image\n",
    "img = cv.imread(os.path.join(os.getcwd(), 'data', 'matterhorn.jpg'))\n",
    "print('Image has dimensions: {}'.format(img.shape))\n",
    "print('There is a total of {} elements'.format(img.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f32870",
   "metadata": {},
   "source": [
    "OpenCV loads the image in a type of variable called a NumPy array. Elements of a NumPy array are accessed through indexing with the operator `[.]` where `.` indicates which dimensions you're accessing. The dimensions are organized as follows: `rows, cols, channels`. More information about indexing in NumPy is in the [doc](https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d26d33",
   "metadata": {},
   "source": [
    "**QUESTION:** (/2)\n",
    "- What do `rows, cols, channels` represent in the image? \n",
    "- What is the value of channels for grayscale and colored image? \n",
    "- Is the `matterhorn.jpg` image a grayscale or color image? Why? \n",
    "- What is the meaning of each channel when the image is loaded with OpenCV ([Hint](https://docs.opencv.org/4.x/d4/da8/group__imgcodecs.html#ga288b8b3da0892bd651fce07b3bbd3a56))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada6c107",
   "metadata": {},
   "source": [
    "**YOUR ANSWER** \n",
    "\n",
    "- `rows` corresponds to the number of pixels from left to right (on the horizontal axis => on a row), `cols` corresponds to the number of pixels from the top to the bottom (on the vertical axis => on a column) and `channels` correspond to the the colour of the pixel. The latter has dimension 3 to store the amount of blue, green and red colour.\n",
    "- Each of the channel has 256 different possible values (from 0 to 255). There is only one channel (value directly witten under image[row, col]) for the grayscale and three for a colour image (1 dimension for blue, one for green, one for red).\n",
    "- It is a colour image since it has 3 channels (one per colour).\n",
    "- For colour image the first channel corresponds to the blue colour, the second channel to the green colour and the third one stands for the red. For grayscale, the channel corresponds to the shade of gray with 255 = white, 0 = black. Anything in between leads to a different shade of gray."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2c532d",
   "metadata": {},
   "source": [
    "In the next cell we display the image using the provided function `display_image()` in file `utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2e8635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image can be displayed like this\n",
    "display_image(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa71cbb5",
   "metadata": {},
   "source": [
    "Now that you have an image loaded in the notebook, let's extract some properties from it. Here is how you can access a pixel or a region of the image, and modify a pixel value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4976248",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_row = 10 \n",
    "pixel_col = 10\n",
    "\n",
    "# accessing pixel (pixel_x, pixel_y) \n",
    "print('Pixel value at location {}x{} is {}'.format(pixel_row, pixel_col, img[pixel_row, pixel_col]))\n",
    "\n",
    "# accessing region of the image starting at(pixel_row, pixel_col)\n",
    "region_size = 10 \n",
    "img_region = img[pixel_row:pixel_row+region_size, pixel_col:pixel_col+region_size]\n",
    "print('Region size is {}'.format(img_region.shape))\n",
    "\n",
    "# modifying pixel value\n",
    "img[pixel_row, pixel_col] = [0, 0, 0]\n",
    "print('New pixel value at location {}x{} is {}'.format(pixel_row, pixel_col, img[pixel_row, pixel_col]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeda73c5",
   "metadata": {},
   "source": [
    "**QUESTION:** (/1)\n",
    "\n",
    "In the next cell, change the color of the pixels to white in a rectangle of size (40, 100) starting from the pixel (30, 60) and show the new image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf75811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modifying pixel value\n",
    "\n",
    "## YOUR CODE HERE\n",
    "\n",
    "## x = rows | y = columns \n",
    "\n",
    "# Location\n",
    "pixel_row = 30 \n",
    "pixel_col = 60\n",
    "\n",
    "# Region size\n",
    "rs1, rs2 = 40, 100 \n",
    "\n",
    "img[pixel_row:pixel_row+rs1, pixel_col:pixel_col+rs2] = [255, 255, 255] \n",
    "\n",
    "# Display the modified image with white rectangle\n",
    "display_image(img);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae38358b",
   "metadata": {},
   "source": [
    "### 1.1.2 Colorspace Conversion\n",
    "\n",
    "Now, you will learn how to convert images from one colorspace to another, like BGR ↔ Gray, BGR ↔ YUV etc. There are more than 150 color-space conversion methods available in OpenCV. \n",
    "\n",
    "**QUESTION:** (/4)\n",
    "\n",
    "In the next two cells, implements the conversion function going from BGR image to GRAY image and BGR to YUV image. There are built-in functions in OpenCV doing exactly that but for the moment we'll ask you to implement your **own** function in order to get the idea of what is happening under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee4756a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bgr_to_grayscale(frame):\n",
    "    \"\"\"\n",
    "    Convert a given bgr image into grayscale image\n",
    "    :param frame: Color image to convert\n",
    "    :return: Grayscale image as numpy array\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # BGR to grayscale using the luminosity method as follows\n",
    "    # Y = 0.299*R + 0.587*G + 0.114*B\n",
    "    row, col = frame.shape[:2]\n",
    "    image = np.zeros(frame.shape[:2])\n",
    "    \n",
    "    for i in range(row):\n",
    "        for j in range(col):\n",
    "            B, G, R = frame[i, j, 0], frame[i, j, 1], frame[i, j, 2]\n",
    "            image[i, j] = 0.299*R + 0.587*G + 0.114*B\n",
    "            \n",
    "    return image.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b406a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bgr_to_yuv(frame):\n",
    "    \"\"\"\n",
    "    Convert a given rgb image into hsv image\n",
    "    :param frame: Color image to convert\n",
    "    :return: YUV image as numpy array\n",
    "    \"\"\"\n",
    "    \n",
    "    img_bgr = np.asanyarray(frame)\n",
    "    img_bgr = img_bgr.astype(float)\n",
    "\n",
    "    # Create an empty YUV image with the same dimensions as the input image\n",
    "    img_yuv = np.zeros_like(img_bgr)\n",
    "\n",
    "    # Convert each pixel from BGR to YUV\n",
    "    for i in range(img_bgr.shape[0]):\n",
    "        for j in range(img_bgr.shape[1]):\n",
    "            # Extract BGR values\n",
    "            b = img_bgr[i,j,0]\n",
    "            g = img_bgr[i,j,1]\n",
    "            r = img_bgr[i,j,2]\n",
    "\n",
    "            # Convert to YUV\n",
    "            # y = 0.299*r + 0.587*g + 0.114*b\n",
    "            # u = -0.14713*r - 0.28886*g + 0.436*b\n",
    "            # v = 0.615*r - 0.51499*g - 0.10001*b\n",
    "            \n",
    "            y = 0.299*r + 0.587*g + 0.114*b\n",
    "            u = (-0.14713*r - 0.28886*g + 0.436*b)/2.25 + 128\n",
    "            v = (0.615*r - 0.51498*g - 0.10001*b)/2.25 +128\n",
    "\n",
    "            # Update YUV values in the output image\n",
    "            img_yuv[i,j,0] = y\n",
    "            img_yuv[i,j,1] = u\n",
    "            img_yuv[i,j,2] = v\n",
    "            \n",
    "    img_yuv = img_yuv.astype(np.uint8)    \n",
    "\n",
    "    return img_yuv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965aca21",
   "metadata": {},
   "source": [
    "The following lines shows the conversion based on your implementation and the built-in function from OpenCV. If everything went well, the two should match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a622d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "img = cv.imread(os.path.join(os.getcwd(), 'data', 'matterhorn.jpg'))\n",
    "\n",
    "\n",
    "# convert to gray + hsv\n",
    "img_gray = convert_bgr_to_grayscale(img)\n",
    "img_yuv = convert_bgr_to_yuv(img)\n",
    "gt_gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "gt_yuv = cv.cvtColor(img, cv.COLOR_BGR2YUV)\n",
    "\n",
    "# Display\n",
    "fig, ax = plt.subplots(2, 3, figsize=(12, 7))\n",
    "display_image(img, axes=ax[0][0])\n",
    "ax[0][0].set_title('Original')\n",
    "display_image(img_gray, axes=ax[0][1])\n",
    "ax[0][1].set_title('Grayscale')\n",
    "display_image(img_yuv, axes=ax[0][2])\n",
    "ax[0][2].set_title('YUV')\n",
    "\n",
    "display_image(img, axes=ax[1][0])\n",
    "ax[1][0].set_title('Original')\n",
    "display_image(gt_gray, axes=ax[1][1])\n",
    "ax[1][1].set_title('OpenCV - Grayscale')\n",
    "display_image(gt_yuv, axes=ax[1][2])\n",
    "ax[1][2].set_title('OpenCV - YUV');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7a3cd8",
   "metadata": {},
   "source": [
    "Now you know how to convert *BGR* image to any color space, you can use this to extract a colored object. In *HSV*, it is more easier to represent a color than in *BGR* colorspace. Here is the method to extract a colored object:\n",
    "\n",
    "1. Reading an input image\n",
    "2. Convert from *BGR* to *HSV* color-space using bluit-in function from OpenCV\n",
    "3. We threshold the *HSV* image to extract the pixels in the desired color range. \n",
    "\n",
    "**QUESTION:** (/2)\n",
    "\n",
    "First, implement the function `threshold_bgr_image()` to threshold the color image in the desired color range. \n",
    "\n",
    "**Hint**: You may use the OpenCV functions `inRange()` and `bitwise_and()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3707aa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_bgr_image(frame, lower, upper):\n",
    "    \"\"\"\n",
    "    Segment image.\n",
    "        1. Convert image to HSV color space\n",
    "        2. Find pixels in the desired color range (lower, upper)\n",
    "        3. Apply the mask to the image. \n",
    "    :param frame: BGR image to segment\n",
    "    :param lower: Lower threshold value\n",
    "    :param upper: Upper threshold value\n",
    "    :return res: Segmented image\n",
    "    :return mask: binary mask\n",
    "    \"\"\"\n",
    "    \n",
    "    hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV)\n",
    "    mask = cv.inRange(hsv, lower, upper)\n",
    "\n",
    "    res = cv2.bitwise_and(frame,frame,mask = mask)\n",
    "    \n",
    "    return res, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632e4658",
   "metadata": {},
   "source": [
    "The next cell test your function on the blue pixels of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51333bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read input\n",
    "img = cv.imread(os.path.join(os.getcwd(),'data', 'circles_color.png'))\n",
    "\n",
    "# 2. Define range of blue color in HSV\n",
    "lower = np.asarray([90,120,120])\n",
    "upper = np.asarray([130,255,255])\n",
    "im, mask = threshold_bgr_image(img, lower, upper)\n",
    "\n",
    "# 3. Display using subplots\n",
    "fig, ax = plt.subplots(1, 3, figsize=(17, 9))\n",
    "display_image(img, axes=ax[0])\n",
    "ax[0].set_title('Input')\n",
    "display_image(mask, axes=ax[1], cmap='gray')\n",
    "ax[1].set_title('Detected Mask')\n",
    "display_image(im, axes=ax[2])\n",
    "ax[2].set_title('Extracted color');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdd6101",
   "metadata": {},
   "source": [
    "In order to find *HSV* values, you can use the same function, `cv.cvtColor()`. Instead of passing an image, you just pass the *BGR* values you want. The next cell shows an example to find the HSV value of red. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325a97f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "red = np.uint8([[[0, 0, 255 ]]])\n",
    "hsv_red = cv.cvtColor(red, cv.COLOR_BGR2HSV)\n",
    "print( 'HSV component {}'.format(hsv_red))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46cc678",
   "metadata": {},
   "source": [
    "**QUESTION:** (/2)\n",
    "\n",
    "Try to extract the two **Yellow** circle. Comment and show the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c33767",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Finding yellow bounds ###\n",
    "\n",
    "yellow_low = np.uint8([[[0, 240, 240 ]]])\n",
    "hsv_yl = cv.cvtColor(yellow_low, cv.COLOR_BGR2HSV)\n",
    "print( 'HSV lower bound for yellow {}'.format(hsv_yl))\n",
    "\n",
    "yellow_up = np.uint8([[[0, 255, 255 ]]])\n",
    "hsv_yu = cv.cvtColor(yellow_up, cv.COLOR_BGR2HSV)\n",
    "print( 'HSV upper bound for yellow {}'.format(hsv_yu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc69b54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read input\n",
    "img = cv.imread(os.path.join(os.getcwd(),'data', 'circles_color.png'))\n",
    "\n",
    "# 2. Extract yellow color in HSV\n",
    "\n",
    "### Your code here ### \n",
    "lower = np.asarray([30, 255, 240]) # Lower bound\n",
    "upper = np.asarray([30, 255, 255]) # Upper bound\n",
    "im, mask = threshold_bgr_image(img, lower, upper)\n",
    "\n",
    "# 3. Display using subplots\n",
    "fig, ax = plt.subplots(1, 3, figsize=(17, 9))\n",
    "display_image(img, axes=ax[0])\n",
    "ax[0].set_title('Input')\n",
    "display_image(mask, axes=ax[1], cmap='gray')\n",
    "ax[1].set_title('Detected Mask')\n",
    "display_image(im, axes=ax[2])\n",
    "ax[2].set_title('Extracted color')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e916ec",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**\n",
    "\n",
    "We can see that the white part of the mask takes the shape of the area that has the corresponding range of colour (in our case a circle inside the range of colour). The other parts where the colour is outside the desired range are set to black. Applying the mask to the image filters out the undesired colours and only draws the one located within the right range of colour."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8636022",
   "metadata": {},
   "source": [
    "So far you have been working with color image, but you could apply the same approach with the grayscale image.\n",
    "\n",
    "**QUESTION:** (/2)\n",
    "\n",
    "Now, try to extract the **yellow** pencil using the grayscale image. What are the pros and cons of each method (colors *vs* grayscale)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9329c9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Shade of gray for yellow ###\n",
    "\n",
    "yellow = np.uint8([[[0, 255, 255 ]]])\n",
    "gray_y = cv.cvtColor(yellow, cv.COLOR_BGR2GRAY)\n",
    "print( 'Grayscale for yellow {}'.format(gray_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b26976",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def threshold_gray_image(frame, lower, upper):\n",
    "    \"\"\"\n",
    "    Segment image.\n",
    "        1. Convert image to gray color space\n",
    "        2. Find pixels in the desired color range (lower, upper)\n",
    "        3. Apply the mask to the image. \n",
    "    :param frame: BGR image to segment\n",
    "    :param lower: Lower threshold value\n",
    "    :param upper: Upper threshold value\n",
    "    :return: Segmented image, binary mask\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    gray_image = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "    mask = cv.inRange(gray_image, lower, upper)\n",
    "\n",
    "    res = cv2.bitwise_and(frame, frame, mask = mask)\n",
    "    \n",
    "    return res, mask\n",
    "\n",
    "# 1. Read input\n",
    "img = cv.imread(os.path.join(os.getcwd(),'data', 'circles_color.png'))\n",
    "\n",
    "# 2. Extract yellow color in gray\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "lower = np.uint8([[[225]]]) ### Lower bound\n",
    "upper = np.uint8([[[227]]]) ### Upper bound\n",
    "im, mask = threshold_gray_image(img, lower, upper)\n",
    "\n",
    "# 3. Display using subplots\n",
    "fig, ax = plt.subplots(1, 3, figsize=(17, 9))\n",
    "display_image(img, axes=ax[0])\n",
    "ax[0].set_title('Input')\n",
    "display_image(mask, axes=ax[1], cmap='gray')\n",
    "ax[1].set_title('Detected Mask')\n",
    "display_image(im, axes=ax[2])\n",
    "ax[2].set_title('Extracted color')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186a3394",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**\n",
    "\n",
    "The *pros* of **colour threshold** is that the method is very accurate and we can extract very specific colours thanks to a judicious choice of the range for the channels. The main *cons* is to guess this range of colour that the method requires to filter out the undesired ones. If we wish to extract a specific type of yellow, we may try several ranges before finding the right one the three channels!\n",
    "\n",
    "The *pros* of **grayscale threshold** is that we only need to find one range of colour (the right shade of gray) versus 3 for *colour threshold*. The main *cons* is that this method is not accurate compared to *colour threshold* which is very specific. Different colours may have the same shade of gray and will be considered as the same colour when the filter is applied. We can see this with the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45db8ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Yellow in BGR [0, 255, 255] transformed in grayscale leads to [226]\n",
    "\n",
    "### Finding a different colour with the same shade of gray as yellow\n",
    "diff_colour = np.uint8([[[160, 245, 215 ]]])\n",
    "diff_colour = np.append(diff_colour, np.uint8([[[0, 255, 255 ]]]), 0) # Add yellow\n",
    "diff_colour = np.append(diff_colour, np.uint8([[[0, 0, 255 ]]]), 0) # Add red\n",
    "\n",
    "### Taking the same bounds for the filter as before\n",
    "lower = np.uint8([[[225]]])\n",
    "upper = np.uint8([[[227]]])\n",
    "\n",
    "im2, mask2 = threshold_gray_image(diff_colour, lower, upper)\n",
    "gray_image = cv.cvtColor(diff_colour, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 7))\n",
    "display_image(diff_colour, axes=ax[0])\n",
    "ax[0].set_title('New_image')\n",
    "display_image(im2, axes=ax[1])\n",
    "ax[1].set_title('Extracted colour')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89caa382",
   "metadata": {},
   "source": [
    "As one can see, red is filtered out as expected but pale green stands within the range of colour of yellow when it is transformed to grayscale. Thus, it is not filtered out and appears on the filtered image where we are supposed to see only yellow and black."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a951e1",
   "metadata": {},
   "source": [
    "### 1.1.3 Image Histogram\n",
    "\n",
    "An histogram is a graph or plot, which gives you an overall idea about the intensity distribution of the pixels of the image. It is just another way of understanding the image. By looking at the histogram of an image, you get intuition about contrast, brightness, intensity distribution etc of that image. Almost all image processing tools today, provides features on histogram.\n",
    "\n",
    "**QUESTION:** (/1)\n",
    "\n",
    "In the next cell, implement a function using OpenCV that computes the image histogram for all channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acf1350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_image_histogram(image):\n",
    "    \"\"\"\n",
    "    Compute image color distribution for each channels of a given `image`\n",
    "    :param image: Image to extract histograms for\n",
    "    :return: List of histogram, one per channel.\n",
    "    \"\"\"\n",
    "    \n",
    "    hist = []\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    if len(image.shape) == 3:\n",
    "        for i in range(image.shape[2]):\n",
    "            hist.append(cv.calcHist([image], channels=[i], mask=None, histSize=[256], ranges=[0, 256]))\n",
    "    elif len(image.shape) == 2:\n",
    "        hist.append(cv.calcHist([image], channels=[0], mask=None, histSize=[256], ranges=[0, 256]))\n",
    "    else:\n",
    "        raise NameError(\"Error with the image dimensions\")\n",
    "        \n",
    "    \"\"\"\"\n",
    "    Implementation from scratch\n",
    "    row, col = image.shape[:2]\n",
    "    \n",
    "    if len(image.shape) == 3: \n",
    "        b, r, g = [0]*256, [0]*256, [0]*256\n",
    "        for i in range(row):\n",
    "            for j in range(col):\n",
    "                k, l, m = image[i, j, 0], image[i, j, 1], image[i, j, 2]\n",
    "                b[k] += 1\n",
    "                g[l] += 1\n",
    "                r[m] += 1 \n",
    "\n",
    "        hist.append(b)\n",
    "        hist.append(g)\n",
    "        hist.append(r)\n",
    "    elif len(image.shape) == 2:\n",
    "        g = [0]*256\n",
    "        for i in range(row):\n",
    "            for j in range(col):\n",
    "                k = image[i, j]\n",
    "                g[k] += 1\n",
    "        hist.append(g)\n",
    "\n",
    "    else:\n",
    "        raise NameError(\"Error with the image dimensions\")\"\"\"\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c15559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load image\n",
    "img = cv.imread(os.path.join(os.getcwd(),'data', 'matterhorn.jpg'))\n",
    "\n",
    "# 2. Compute histogram for each channels\n",
    "hists = compute_image_histogram(img)\n",
    "\n",
    "# 3. Plot \n",
    "color = ['b', 'g', 'r']\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 7))\n",
    "display_image(img, axes=ax[0])\n",
    "for k, hist in enumerate(hists):\n",
    "    ax[1].plot(hist, color=color[k])\n",
    "ax[1].set_title('Color Histogram by channels')\n",
    "ax[1].set_xlim([0, 256])\n",
    "ax[1].set_xlabel('Pixel Intensity')\n",
    "ax[1].set_ylabel('Number of Pixel')\n",
    "plt.legend(['Blue Channel', 'Green Channel', 'Red Channel'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780c6def",
   "metadata": {},
   "source": [
    "You can see the image and the corresponding histograms.\n",
    "\n",
    "**QUESTION:** (/1)\n",
    "\n",
    "What is the significant of the pixel intensity?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c3b507",
   "metadata": {},
   "source": [
    "**YOUR ANSWER** \n",
    "\n",
    "It describes how bright the pixel will be. A high intensity leads to a bright pixel (with 255 as maximum) while a low intensity leads to a dark pixel (with 0 as minimum). The combination of the values of the channels determines the final intensity and the colour of the pixel. For instance taking a pixel with the blue channel set to 0 and the green and red ones set to 255 will produce bright yellow. This means that only green and red contribute with full intensity to the colour.\n",
    "One can also observe this with the example below. We take first blue colour set to 255 (R and G set to 0), then we set it to a lower intesity (150) and it becomes darker. Finally, we set blue to 255 as well as red and we observe a new colour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05357613",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1, c2 = np.uint8([[[255, 0, 0 ]]]), np.uint8([[[150, 0, 0 ]]]) # Reducing pixel intensity\n",
    "c3 = np.uint8([[[255, 0, 255 ]]]) \n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(10, 7))\n",
    "display_image(c1, axes=ax[0])\n",
    "ax[0].set_title('Original colour')\n",
    "display_image(c2, axes=ax[1])\n",
    "ax[1].set_title('Lower intensity')\n",
    "display_image(c3, axes=ax[2])\n",
    "ax[2].set_title('Increasing intensity on another channel')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3eec22",
   "metadata": {},
   "source": [
    "#### Histogram equalization\n",
    "Consider an image whose pixel values are confined to some specific range of values only. For instance, brighter image will have all pixels confined to high values. But a good image will have pixels from all regions of the image. So you need to stretch this histogram to either ends and that is what Histogram Equalization does. This normally improves the contrast of the image ([doc](https://en.wikipedia.org/wiki/Histogram_equalization)).\n",
    "\n",
    "**QUESTION:** (/3)\n",
    "\n",
    "In the next section implement such contrast enhancement function (i.e. based on histogram equalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c226806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrast_enhancement(image):\n",
    "    \"\"\"\n",
    "    Compute histogram and apply histogram equalization on a given image.\n",
    "    :param image: Image to enhance contrast\n",
    "    :return: tuple: enhanced image, input image histogram, transformed image histogram\n",
    "    \"\"\"\n",
    "    \n",
    "    hist = None\n",
    "    hist_equ = None\n",
    "    image_equ = None\n",
    "    \n",
    "    hist = compute_image_histogram(image)[0]\n",
    "\n",
    "    image_equ = cv.equalizeHist(image)\n",
    "    hist_equ = cv.calcHist([image_equ],[0],None,[256],[0,256])\n",
    "    hist = np.array(hist)\n",
    "    \n",
    "    return image_equ, hist, hist_equ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1122a845",
   "metadata": {},
   "source": [
    "The next cell shows you the results of histogram equalization on an image. \n",
    "\n",
    "**QUESTION:** (/2)\n",
    "\n",
    "- How has the contrast evolved after applying the transformation ? \n",
    "- How has the histogram changed ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c9aee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "img = cv.imread(os.path.join(os.getcwd(),'data', 'matterhorn_snow.jpg'), cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Enhance contrast with histogram equalization\n",
    "img_equ, hist, hist_equ = contrast_enhancement(img)\n",
    "\n",
    "# Display\n",
    "fig, ax = plt.subplots(2, 2, figsize=(17, 9))\n",
    "display_image(img, axes=ax[0][0])\n",
    "ax[0][0].set_title('Original')\n",
    "# Histogram original\n",
    "ax[0][1].bar(np.arange(hist.shape[0]), hist.reshape(-1),  width=1)\n",
    "ax[0][1].set_xlim([0, 256])\n",
    "ax[0][1].set_title('Histogram')\n",
    "# Histogram equalized\n",
    "display_image(img_equ, axes=ax[1][0])\n",
    "ax[1][0].set_title('Histogram Equalized')\n",
    "ax[1][1].bar(np.arange(hist_equ.shape[0]), hist_equ.reshape(-1),  width=1)\n",
    "ax[1][1].set_xlim([0, 256])\n",
    "ax[1][1].set_title('Histogram')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76afa51",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**\n",
    "\n",
    "- The contrast is much better with the equalized histograms especially for bright area of the original picture. We can better distinguish the clouds for instance. In the first picture, these can hardly be distinguished due to the high intensity of the sky while we can easily observe them in the second picture. However, darker areas such as the forest have become darker and it is harder to distinguish trees on the second picture.\n",
    "\n",
    "- The equalized histgram changes smoothly and has a better repartition of the pixles over the intensity scale (from 0 to 255). The shape is conserved but the histogram has been streched over the full scale. Higher intensity areas (from 150 to 250) are more sparsed while low intensity areas (from 0 to 50) are more dense. This explains why clouds can be easily distinguished as they are bright while the forest not since it is dark. The contrast is better in the region where the histogram is sparsed since adjacent pixels will probably have different intensity. If the histogram is more dense, adjacent pixels may be also be adjacent in the histogram i. e. would roughly have the same intensity and lead to a lower contrast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe04cb63",
   "metadata": {},
   "source": [
    "### 1.2 Image filtering and edges detection\n",
    "\n",
    "(50 points)\n",
    "\n",
    "Image filtering is a two-dimensional convolution of the image by a small matrix called kernel. The choice of the kernel depends on the goal of the filtering operation, which can be noise removal, edge detection, contrast enhancement, etc. In general, filters are classified into two families: low-pass filters and high-pass filters. \n",
    " \n",
    "### 1.2.1 Low-pass filters \n",
    "\n",
    "The most common low-pass filters are the [averaging filter](https://docs.opencv.org/4.x/d4/d86/group__imgproc__filter.html#ga8c45db9afe636703801b0b2e440fce37), the [Gaussian filter](https://docs.opencv.org/4.x/d4/d86/group__imgproc__filter.html#gaabe8c836e97159a9193fb0b11ac52cf1) and the [median filter](https://docs.opencv.org/4.x/d4/d86/group__imgproc__filter.html#ga564869aa33e58769b4469101aac458f9). \n",
    "\n",
    "**QUESTION:** (/3)\n",
    "\n",
    "In the next cell, implement the function `apply_lowpass_filter()`, which takes as input an image, and the filter parameters in a dictionary (filter type and size) and returns the filtered image. Use the corresponding OpenCV functions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9b4863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lowpass_filter(img, filter_params):\n",
    "    \"\"\"\n",
    "    Apply a low-pass filter on a given image.\n",
    "    :param image: Image to enhance contrast\n",
    "    :param filter_params: Parameters of the filter\n",
    "    :return: img_filt: filtered image\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    ftype = filter_params[\"ftype\"]\n",
    "    fsize = filter_params[\"fsize\"]\n",
    "    \n",
    "    if ftype == 'averaging':\n",
    "        img_filt = cv.blur(img, (fsize,fsize))\n",
    "    elif ftype == 'gaussian':\n",
    "        img_filt = cv.GaussianBlur(img, (fsize, fsize), 0)\n",
    "    elif ftype == 'median': \n",
    "        img_filt = cv.medianBlur(img, fsize)\n",
    "    else:\n",
    "        raise NameError(\"Error. Wrong filter selected.\")\n",
    "    \n",
    "    return img_filt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a28e370",
   "metadata": {},
   "source": [
    "The next cell tests your implementation on two images corrupted by a different noise, with a filter of size (7x7). \n",
    "\n",
    "**QUESTION:** (/2)\n",
    "\n",
    "What is the effect of each filter? How different are the filtered images? Which filter is the best for each image and why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5da46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "imgs = [cv.imread(os.path.join(os.getcwd(),'data', 'matterhorn_noise1.jpg'), cv.IMREAD_GRAYSCALE), cv.imread(os.path.join(os.getcwd(),'data', 'matterhorn_noise2.jpg'), cv.IMREAD_GRAYSCALE)]\n",
    "img_title = [\"Salt & Pepper\", \"Gaussian\"]\n",
    "\n",
    "# Filtering with a kernel of size 7x7\n",
    "filter_size = 7\n",
    "filter_params = [ {\"ftype\":\"averaging\", \"fsize\":filter_size},\n",
    "                 {\"ftype\":\"gaussian\", \"fsize\":filter_size},\n",
    "                 {\"ftype\":\"median\", \"fsize\":filter_size}]\n",
    "\n",
    "\n",
    "# Display image\n",
    "fig, ax = plt.subplots(len(filter_params)+1, len(imgs), figsize=(20, 20))\n",
    "\n",
    "for curr_img_idx, curr_img in enumerate(imgs):\n",
    "    display_image(curr_img, axes=ax[0, curr_img_idx], cmap='gray')\n",
    "    \n",
    "    ax[0, curr_img_idx].set_title(img_title[curr_img_idx], fontsize=30)\n",
    "\n",
    "    for curr_filt_idx, curr_filt in enumerate(filter_params):        \n",
    "        img_filt = apply_lowpass_filter(curr_img, curr_filt)\n",
    "        display_image(img_filt, axes=ax[curr_filt_idx+1, curr_img_idx], cmap='gray')\n",
    "        ax[curr_filt_idx+1, curr_img_idx].set_title(curr_filt[\"ftype\"], fontsize=30)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4cc322",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**\n",
    "\n",
    "***What is the effect of each filter? How different are the filtered images?***\n",
    "\n",
    "- Averaging filter: This filter replaces each pixel with the average value of its neighboring pixels. It can smooth out an image by reducing noise and blurring the edges of objects. For S&P, the filter is not useful in terms of reducing noise. For Gaussian noise, at the filter size 7 it smoothes the noise considerable, though at a cost of blurring the object, losing details and information.\n",
    "- Gaussian filter: This filter applies a weighted average to the neighboring pixels, with weights determined by a Gaussian distribution. It also smoothes out the image by reducing noise while preserving edges and details better than the averaging filter for the gaussian noise image. For S&P, it is not useful. \n",
    "- Median filter: This filter replaces each pixel with the median value of its neighboring pixels. The resulting image when applied median filter with size 7 in both cases loses significant details. For S&P, the noise can be seen to be removed almost completely while you can still observe a small level of noise in gaussian noise case, it still smoothes the noise as compared to the original noisy image.\n",
    "\n",
    "\n",
    "***Which filter is the best for each image and why?***\n",
    "\n",
    "For Salt and paper image, the best filter is the median one since it is particularly effective in removing salt-and-pepper noise, which appears as isolated bright or dark pixels in the image. The median filter is resilient against outlier/impulse type of noise.\n",
    "\n",
    "For Gaussian image the best filter is the gaussian one since it balances the noise smoothing quality and the trade-off of losing image quality. As compared to the other 2 filters, both would lose detail to achieve the same level of noise smoothing gaussian filter provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b592ac5",
   "metadata": {},
   "source": [
    "Now that you are able to filter an image, let's find the optimal kernel. In this question, we consider the optimal filter to be the one that produces a filtered image that is visually closest to the image without noise.\n",
    "\n",
    "**QUESTION:** (/4)\n",
    "\n",
    "For each image, propose an optimal kernel (type and size), justify your choice and show the filtered image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1f4aa6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load image\n",
    "img_orig = cv.imread(os.path.join(os.getcwd(),'data', 'matterhorn.jpg'), cv.IMREAD_GRAYSCALE)\n",
    "imgs = [cv.imread(os.path.join(os.getcwd(),'data', 'matterhorn_noise1.jpg'), cv.IMREAD_GRAYSCALE), cv.imread(os.path.join(os.getcwd(),'data', 'matterhorn_noise2.jpg'), cv.IMREAD_GRAYSCALE)]\n",
    "img_title = [\"Salt & Pepper\", \"Gaussian\"]\n",
    "\n",
    "# Best filter parameters\n",
    "\n",
    "\n",
    "###################\n",
    "# YOUR CODE HERE\n",
    "best_filter_params_1 = {\"ftype\": \"median\", \"fsize\": 3}\n",
    "best_filter_params_2 = {\"ftype\": \"gaussian\", \"fsize\": 7}\n",
    "###################\n",
    "\n",
    "\n",
    "# Filtering\n",
    "img1_best = apply_lowpass_filter(imgs[0], best_filter_params_1)\n",
    "img2_best = apply_lowpass_filter(imgs[1], best_filter_params_2)\n",
    "\n",
    "# Display image\n",
    "fig, ax = plt.subplots(1, 3, figsize=(30, 10))\n",
    "display_image(img_orig, axes=ax[0], cmap='gray')\n",
    "ax[0].set_title(\"Orignal image\", fontsize=30)\n",
    "display_image(img1_best, axes=ax[1], cmap='gray')\n",
    "ax[1].set_title(\"Best filter - S&P Noise\", fontsize=30)\n",
    "display_image(img2_best, axes=ax[2], cmap='gray')\n",
    "ax[2].set_title(\"Best filter - Gaussian noise\", fontsize=30)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e37b45-f986-4b1c-95ca-16139ae530e7",
   "metadata": {},
   "source": [
    "***For each image, propose an optimal kernel (type and size), justify your choice and show the filtered image.***\n",
    "\n",
    "For S&P, my goal is to remove the noise while still keeping image details, and the best filter is median with size 3. The other 2 filters is not effective in removing s&p noise. If we increase the size greater than 3, we start losing details in the image. Fixing the size to 3 achieve the tasking of clearing most noise while only suffer from minor details loss. Image quality is still comparable to the original image, though under close inspection you can still see that finer details loss. (regions leading up to the peak)\n",
    "\n",
    "For Gaussian noise, there is a trade-off to be made: noise smoothing and image blur. The best filter I choose is Gaussian with size 7. Decreasing the size would result in more noticiable noise in the image while larger size loses too many details. The median filter would lose too much detail for the same amount of noise reduction. For the average filter, the size 5 seems like a good candidate but under side by side eye inspection, the gaussian filter size 7 reduces more noise while keeping the same amount of detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb3f7c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 1.2.2 High-pass filters\n",
    "\n",
    "The second family of filters are the high-pass filters. You will try two of them: the first-order [Sobel filter](https://docs.opencv.org/3.4/d4/d86/group__imgproc__filter.html#gacea54f142e81b6758cb6f375ce782c8d) and the second order [Laplacian filter](https://docs.opencv.org/3.4/d4/d86/group__imgproc__filter.html#gad78703e4c8fe703d479c1860d76429e6).\n",
    "\n",
    "We start with the first-order Sobel filter, more info is available in the [doc](https://docs.opencv.org/3.4/d4/d86/group__imgproc__filter.html#gacea54f142e81b6758cb6f375ce782c8d). When using first-order filters, the kernel is applied in one direction, along the rows or the columns of the image. \n",
    "\n",
    "**QUESTION:** (/2)\n",
    "\n",
    "In the next cell, implement the function `apply_sobel_filter` which takes as input an image, the\n",
    "direction of filtering (`x` or `y`), and the filter size  and returns the filtered image. Use the corresponding OpenCV function with the parameter `ddepth=cv.CV_64F`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdab0db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sobel_filter(img, direction, filter_size):\n",
    "    \n",
    "    \"\"\"\n",
    "    Apply the sobel filter on a given image.\n",
    "    :param img: Image to enhance contrast\n",
    "    :param direction: Direction of the derivative\n",
    "    :param filter_size: Size of the filter\n",
    "    :return: img_filt: filtered image\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE \n",
    "    if direction not in [\"x\", \"y\"]:\n",
    "        raise ValueError(\"Direction invalid. Must be \\\"x\\\" or \\\"y\\\".\")\n",
    "    if direction == 'x':\n",
    "        img_filt = cv.Sobel(img, cv.CV_64F, 1, 0, ksize=filter_size)\n",
    "    else:\n",
    "        img_filt = cv.Sobel(img, cv.CV_64F, 0, 1, ksize=filter_size)\n",
    "    \n",
    "    return img_filt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b465ac45",
   "metadata": {},
   "source": [
    "The next cell tests your implementation with a filter of size (7x7) in both directions. \n",
    "\n",
    "**QUESTION:** (/2)\n",
    "\n",
    "What are the differences between the filtered images? Why? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353f084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "img = cv.imread(os.path.join(os.getcwd(),'data', 'matterhorn.jpg'), cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Sobel filtering\n",
    "filter_size = 7\n",
    "\n",
    "img_sobel_dx = apply_sobel_filter(img, \"x\", filter_size)\n",
    "img_sobel_dy = apply_sobel_filter(img, \"y\", filter_size)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "display_image(img_sobel_dx, axes=ax[0], cmap='gray')\n",
    "ax[0].set_title(\"Direction - dx\", fontsize=30)\n",
    "display_image(img_sobel_dy, axes=ax[1], cmap='gray')\n",
    "ax[1].set_title(\"Direction - dy\", fontsize=30)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae47e6bb",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**\n",
    "\n",
    "The first image emphasizes the changes in pixel values along the x axis. This means that we can easily observe vertical features such as the edges of the Matterhorn that are darker. \n",
    "\n",
    "The second picture emphasizes the horizonal features i.e. perpendicular to the ones on the first picture. Thus, the edges of the Matterhorn that are rather vertical are weaker than on picture one but we can observe the horizontal line between the lake and the shore. The latter cannot be observed on the first picture. \n",
    "\n",
    "As for the features on the mountain (the features we see withing the mountain regions), they also appear very different depending on which edges they have (either vertical or horizontal). \n",
    "\n",
    "The homogeneous areas where the pixel values doesn't change much such as the sky are also similar areas (the clouds and water reflection) on the two filtered images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b58189",
   "metadata": {},
   "source": [
    "Now, you will implement a second-order high-pass filter, the [Laplacian filter](https://docs.opencv.org/3.4/d4/d86/group__imgproc__filter.html#gad78703e4c8fe703d479c1860d76429e6). \n",
    "\n",
    "**QUESTION:** (/1)\n",
    "\n",
    "In the next cell, implement the function `apply_laplacian_filter` which takes as input an image, and the filter size and returns the filtered image. Use the corresponding OpenCV function with the parameter `ddepth=cv.CV_64F`.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76ab7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_laplacian_filter(img, filter_size):\n",
    "    \"\"\"\n",
    "    Apply a laplace filter on a given image.\n",
    "    :param img: Image to enhance contrast\n",
    "    :param filter_size: Size of the filter\n",
    "    :return: img_filt: filtered image\n",
    "    \"\"\"\n",
    "        \n",
    "    # Laplacian filtering\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    img_filt = cv.Laplacian(img, cv.CV_64F, ksize=filter_size)\n",
    "    # img_filt = np.uint8(np.absolute(img_filt))\n",
    "    \n",
    "    return img_filt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d4a130",
   "metadata": {},
   "source": [
    "The next cell tests your implementation with filters of size (1x1), (5x5) and (11x11). \n",
    "\n",
    "**QUESTION:** (/2)\n",
    "\n",
    "What are the differences between the filtered images? Why? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08354d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "img = cv.imread(os.path.join(os.getcwd(),'data', 'matterhorn.jpg'), cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Laplacian filtering\n",
    "filter_size = [1, 5, 11]\n",
    "\n",
    "fig, ax = plt.subplots(1, len(filter_size), figsize=(10*len(filter_size), 10))\n",
    "\n",
    "for curr_filt_idx, curr_filt_size in enumerate(filter_size):\n",
    "    img_laplacian = apply_laplacian_filter(img, curr_filt_size)\n",
    "    display_image(img_laplacian, axes=ax[curr_filt_idx], cmap='gray')\n",
    "    ax[curr_filt_idx].set_title(\"Filter size : {}\".format(curr_filt_size), fontsize=30)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179f8688",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**\n",
    "\n",
    "The bigger the filter size is, the more features we distinguish in the image. The first picture we can hardly see the Matterhorn, because the edges features are very thin and the shades of gray are very homogenous (they differ slighly). With a filter of size 5, we can better distinguish the features that are sharp. For the third image, the overall contrast is better but the image is a little bit blurred compared to the second one.\n",
    "\n",
    "All these changes are due to the filter size. A small filter size removes less noise than a larger one. This means that with the 1x1 filter less features will be extracted than with the 11x11. However, larger filter size leads to artifacts. This is why the third image is a little bit blurred. The second image is an example of the trade off between feature extraction and quality of the image. With this 5x5 size, enough features are extracted to distinguish the basis of the image but the filter is small enough to avoid artifacts leading to a blurred image.\n",
    "\n",
    "In general, when using a smaller filter size, the filtered image will appear sharper and more detailed, but it may also be more sensitive to noise and produce more false positives. Conversely, when using a larger filter size, the filtered image will appear smoother and less detailed, but it may also be more robust to noise and produce fewer false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe85b56",
   "metadata": {},
   "source": [
    "It is convenient to have one function for applying a high-pass filter, similarly to the function `apply_lowpass_filter()`. \n",
    "\n",
    "**QUESTION:** (/2)\n",
    "\n",
    "In the next cell, implement the function `apply_highpass_filter()`, which takes as input an image and a dictionary of the filter parameters, and returns the filtered image. Use the functions you implemented in the previous cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d74020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_highpass_filter(img, filter_params):\n",
    "    \n",
    "    \"\"\"\n",
    "    Apply a highpass filter on a given image.\n",
    "    :param img: Image to enhance contrast\n",
    "    :param filter_params: Parameters of the filter\n",
    "    :return: img_filt: filtered image\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    ftype = filter_params[\"ftype\"]\n",
    "    if ftype not in [\"sobel\", \"laplacian\"]:\n",
    "        raise ValueError(\"Filter invalid. Must be \\\"sobel\\\" or \\\"laplacian\\\".\")\n",
    "        \n",
    "    fsize = filter_params[\"fsize\"]\n",
    "    if ftype == \"sobel\":\n",
    "        direction = filter_params[\"direction\"]\n",
    "        img_filt = apply_sobel_filter(img, direction, fsize)\n",
    "    else:\n",
    "        img_filt = apply_laplacian_filter(img, fsize)\n",
    "    \n",
    "    return img_filt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e58ae4",
   "metadata": {},
   "source": [
    "The next cell tests your implementation with an example of each high-pass filter. \n",
    "\n",
    "**QUESTION** (/3)\n",
    "\n",
    "Comment the differences between the images. \n",
    "\n",
    "**YOUR ANSWER**\n",
    "\n",
    "- The Sobel filter emphasizes edges in vertical direction. The resulting image shows strong vertical edges, such as the lines of the mountain ridges, while horizontal edges are suppressed. The edges look more pronounced in the comparison.\n",
    "\n",
    "- The Laplacian filter emphasizes edges regardless of direction by computing the second derivative of the image. The resulting image shows edges in all directions, including diagonal and circular edges. It also appears sharper and more detailed (mainly because it has information about edges in both directions) in this example, though you might have to inspect carefully to see the detail because there gray shades are harder to see compared to ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8855844b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "img = cv.imread(os.path.join(os.getcwd(),'data', 'matterhorn.jpg'), cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "\n",
    "filter_size = 3\n",
    "\n",
    "# Sobel filtering\n",
    "sobel_params = {\"ftype\":\"sobel\", \"direction\": \"x\", \"fsize\": filter_size}\n",
    "\n",
    "# Laplacian filtering \n",
    "laplacian_params = {\"ftype\":\"laplacian\", \"fsize\": filter_size}\n",
    "\n",
    "highpass_params = [sobel_params, laplacian_params]\n",
    "\n",
    "fig, ax = plt.subplots(1, len(highpass_params), figsize=(10*len(highpass_params), 10))\n",
    "\n",
    "for curr_highpass_idx, curr_highpass_params in enumerate(highpass_params):\n",
    "    img_filt = apply_highpass_filter(img, curr_highpass_params)\n",
    "    display_image(img_filt, axes=ax[curr_highpass_idx], cmap='gray')\n",
    "    ax[curr_highpass_idx].set_title(\" Filter: {}\".format(curr_highpass_params[\"ftype\"]), fontsize=30)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9497a17",
   "metadata": {},
   "source": [
    "### 1.2.3 Image Thresholding\n",
    "\n",
    "Image tresholding consist in assigning a new value to a pixel given a threhsold. If the pixel value is greater than the threshold value, it is assigned one value (*maybe white*), else it is assigned another value (*maybe black*). The function used is `cv.threshold(<image>, <thresh>, <maxVal>, <type>)`. First argument is the source image, which should be a grayscale image. Second argument is the threshold value which is used to classify the pixel values. Third argument is the maxVal which represents the value to be given if pixel value is more than (sometimes less than) the threshold value. OpenCV provides different styles of thresholding and it is decided by the fourth parameter of the function. Different types are:\n",
    "\n",
    "1. `cv.THRESH_BINARY`\n",
    "2. `cv.THRESH_BINARY_INV`\n",
    "3. `cv.THRES_TRUNC`\n",
    "4. `cv.THRESH_TOZERO`\n",
    "5. `cv.THRESH_TOZERO_INV`\n",
    "\n",
    "**QUESTION:** (/2)\n",
    "\n",
    "In the next cell, implement the thresholding operations within the function `threshold_image()`. It takes as input a grayscale image and a dictionary of parameters for the OpenCV function `cv.threshold(<image>, <thresh>, <maxVal>, <type>)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71447f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_image(img, thresh_params):\n",
    "    \"\"\"\n",
    "    Apply different type of threshold to a given image + threshold value\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    Apply a threshold on a given image.\n",
    "    :param image: Image to enhance contrast\n",
    "    :param thresh_params: Parameters of the thresholdinf\n",
    "    :return: img_thresh: thresholded image\n",
    "    \"\"\"    \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    #Convert the image to grayscale if it is not already grayscale\n",
    "    if len(img.shape) == 3:\n",
    "        img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Extract the threshold parameters from the input dictionary\n",
    "    thresh_value = thresh_params.get('thresh')\n",
    "    max_value = thresh_params.get('value')\n",
    "    threshold_type = thresh_params.get('ttype')\n",
    "\n",
    "    # Apply the threshold\n",
    "    ret, img_thresh = cv.threshold(img, thresh_value, max_value, threshold_type)\n",
    "    \n",
    "    return img_thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c53ecb",
   "metadata": {},
   "source": [
    "The next cell tests your implementation. \n",
    "\n",
    "**QUESTION:** (/2)\n",
    "\n",
    "Comment the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a271f911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "img = cv.imread(os.path.join(os.getcwd(),'data', 'grad.png'), cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Various threshold output\n",
    "thresh  = 127 \n",
    "tvalue  = 255\n",
    "\n",
    "thresh_type = [cv.THRESH_BINARY, cv.THRESH_BINARY_INV, cv.THRESH_TRUNC, cv.THRESH_TOZERO, cv.THRESH_TOZERO_INV]\n",
    "\n",
    "thresh_titles = ['BINARY','BINARY_INV','TRUNC','TOZERO','TOZERO_INV']\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(2, 3, figsize=(17, 9))\n",
    "display_image(img, axes=ax[0][0], cmap='gray')\n",
    "ax[0][0].set_title(\"Original image\")\n",
    "\n",
    "for curr_thresh_idx, curr_tthresh in enumerate(thresh_type):\n",
    "\n",
    "    thresh_params = {\"ttype\": curr_tthresh, 'thresh':thresh, 'value': tvalue}\n",
    "    thresh_img = threshold_image(img, thresh_params)\n",
    "\n",
    "    # Draw each samples\n",
    "    k=curr_thresh_idx+1\n",
    "    r = int(k / 3)\n",
    "    c = k % 3\n",
    "    display_image(thresh_img, axes=ax[r][c], cmap='gray')\n",
    "    ax[r][c].set_title(thresh_titles[curr_thresh_idx])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922522be",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**\n",
    "- The BINARY threshold replaces the every pixel value that is greater than the **threshold** with a specified **value** given as an input, and set the remaining pixels to 0\n",
    "- The BINARY_INV threshold does the oposite. It sets all the value greater than the **threshold** to 0, and the rest to the **specified value**\n",
    "- The TRUNC threshold sets all the pixel values greater than the **threshold** to the **specifed value** and keep the rest untouched\n",
    "- The TOZERO threshold sets all values smaller than **threshold** to 0, and keep the rest untouched\n",
    "- The TOZERO_INV sets all values greater than the **threshold** to 0, and keep the rest untouched"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685614b1",
   "metadata": {},
   "source": [
    "\n",
    "#### Otsu Binarization\n",
    "\n",
    "In global thresholding, we used an arbitrary value for threshold value. How can we know a value we selected is good or not? Answer is, trial and error method. But consider an image whose histogram has two peaks (a bimodal image). For that image, we can approximately take a value in the middle of those peaks as threshold value. That is what Otsu binarization does. In simple words, it automatically calculates a threshold value from image histogram for a bimodal image. (For images which are not bimodal, binarization won’t be accurate.)\n",
    "\n",
    "For this, our `cv.threshold()` function is used, but pass an extra flag, `cv.THRES_OTSU`. For threshold value, simply pass zero. Then the algorithm finds the optimal threshold value and returns you as the second output, retVal. If Otsu thresholding is not used, retVal is same as the threshold value you used.\n",
    "\n",
    "\n",
    "Check out below example. Input image is a noisy image. In first case, a global thresholding value of 127 is applied. In second case, Otsu’s thresholding is applied directly. In third case, the image is filtered with a 5x5 gaussian kernel first to remove the noise, then applied Otsu thresholding. See how noise filtering improves the result.\n",
    "\n",
    "**QUESTIONS:** (/5)\n",
    "- For the global thresholding (case 1), why did we chose a value of 127? What would be the effect of a smaller threshold ?  Whats is the value of Otsu thresholding?\n",
    "- What would be the effect of a bigger Gaussian filter, for example 9x9 pixels? \n",
    "- In this example, Otsu's thresholding gives very good results. Can you describe an example for which this method fails? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac1b141",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load image\n",
    "img = cv.imread(os.path.join(os.getcwd(),'data', 'noisy2.png'), cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "tvalue = 255\n",
    "\n",
    "# 1. Global thresholding\n",
    "thresh_params = {\"ttype\": cv.THRESH_BINARY, 'thresh':127, 'value': tvalue}\n",
    "img_tglobal = threshold_image(img, thresh_params)\n",
    "\n",
    "# 2. Otsu thresholding\n",
    "thresh_otsu_params = {\"ttype\": cv.THRESH_BINARY+cv.THRESH_OTSU, 'thresh':0, 'value': tvalue}\n",
    "img_totsu = threshold_image(img, thresh_otsu_params)\n",
    "\n",
    "# 3. Gaussian blurring and Otsu thresholding\n",
    "thresh_params = {\"ttype\": cv.THRESH_BINARY+cv.THRESH_OTSU, 'thresh':0, 'value': tvalue}\n",
    "blur = cv.GaussianBlur(img, (5,5), 0)\n",
    "img_blur_totsu = threshold_image(blur, thresh_params)\n",
    "\n",
    "# Compute histograms\n",
    "hist_g = cv.calcHist([img], channels=[0], mask=None, histSize=[256], ranges=[0, 256])\n",
    "hist_f = cv.calcHist([blur], channels=[0], mask=None, histSize=[256], ranges=[0, 256])\n",
    "\n",
    "# plot all the images and their histograms\n",
    "images = [img, hist_g, img_tglobal,\n",
    "          img, hist_g, img_totsu,\n",
    "          blur, hist_f, img_blur_totsu]\n",
    "titles = ['Original Noisy Image', 'Histogram', 'Global Thresholding (v=127)',\n",
    "          'Original Noisy Image', 'Histogram', \"Otsu's Thresholding\",\n",
    "          'Gaussian filtered Image', 'Histogram', \"Otsu's Thresholding\"]\n",
    "\n",
    "# Display\n",
    "fig, ax = plt.subplots(3, 3, figsize=(17, 9))\n",
    "for k in range(9):\n",
    "    r = int(k / 3)\n",
    "    c = k % 3\n",
    "    \n",
    "    if c != 1:\n",
    "        # Show image\n",
    "        display_image(images[k], axes=ax[r][c], cmap='gray')\n",
    "        ax[r][c].set_title(titles[k])\n",
    "    else:\n",
    "        # Draw histogram\n",
    "        ax[r][c].bar(np.arange(images[k].shape[0]), images[k].reshape(-1), width=1)\n",
    "        ax[r][c].set_title(titles[k])\n",
    "        ax[r][c].set_xlabel('Pixel value')\n",
    "        ax[r][c].set_ylabel('Pixel count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf486529",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**\n",
    "\n",
    "- For the global thresholding (case 1), we chose a value of 127 as it was approximately in the middle of the intensity range of the image. A smaller threshold value would result in more of the image being classified as foreground, leading to more noise in the output. The value of Otsu thresholding is found automatically by the Otsu algorithm, being 114.0\n",
    "\n",
    "- Increasing the Gaussian filter size (e.g., from 5x5 to 9x9 pixels) would result in a smoother image (less noise, histogram becomes tighter (higher peak at the bimodal, ), but it could also lead to loss of image details and edges, and therefore, it might affect the thresholding results.\n",
    "\n",
    "- Otsu's thresholding is a good method for images with a clear bimodal histogram. However, in cases where the histogram of the image is not bimodal, Otsu's method could fail to produce good results. Additionally, if the image contains regions with similar intensity values that belong to different objects or backgrounds, Otsu's method could fail to separate these regions accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423e96b1",
   "metadata": {},
   "source": [
    "### 1.2.4 Edge detection and denoising\n",
    "\n",
    "Edge detection is an application of filtering, which consists in detecting the contours of the objects in the image. \n",
    "In this application, a low-pass filter, a high-pass filter and a binary thresholding operation are often used consecutively. \n",
    "\n",
    "We start with a naive approach to detect edges, a combination of the Sobel high-pass filter and a binary thresholding.  \n",
    "\n",
    "**QUESTION:** (/2)\n",
    "\n",
    "In the next cell, implement the function `find_edges_naive()` with the following operations:\n",
    "- Use the function `apply_highpass_filter()` \n",
    "- Take the absolute value of the filtered image\n",
    "- Convert it to a uint8-grayscale image\n",
    "- Use the function `threshold_image()`on this grayscale image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b39cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_edges_naive(img, highpass_params, thresh_params):\n",
    "    \n",
    "    \"\"\"\n",
    "    Find edges on a given image.\n",
    "    :param image: Image to enhance contrast\n",
    "    :param highpass_params: Parameters of the high-pass filter\n",
    "    :param thresh_params: Parameters of the thresholding operation\n",
    "    :return: img_edges: Edges of the image\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    img_flt = apply_highpass_filter(img,highpass_params)\n",
    "    img_flt = np.abs(img_flt).astype(np.uint8)\n",
    "    img_edges = threshold_image(img_flt,thresh_params)\n",
    "    \n",
    "    return img_edges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad30697",
   "metadata": {},
   "source": [
    "The next cell tests your implementation with a Sobel filter of size (3x3) in the x-direction and a binary threshold.\n",
    "\n",
    "**QUESTION:** (/2)\n",
    "\n",
    "Comment the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e862a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load image\n",
    "imgs = [cv.imread(os.path.join(os.getcwd(),'data', 'matterhorn.jpg'), cv.IMREAD_GRAYSCALE), \n",
    "        cv.imread(os.path.join(os.getcwd(),'data', 'matterhorn_noise1.jpg'), cv.IMREAD_GRAYSCALE),\n",
    "        cv.imread(os.path.join(os.getcwd(),'data', 'matterhorn_noise2.jpg'), cv.IMREAD_GRAYSCALE)]\n",
    "\n",
    "imgs_name = [\"Noiseless\", \"S&P Noise\", \"Gaussian Noise\"]\n",
    "        \n",
    "filter_size = 3\n",
    "thresh = 50 \n",
    "tvalue = 255\n",
    "\n",
    "highpass_params = {\"ftype\":\"sobel\", \"direction\":\"x\", \"fsize\": filter_size}\n",
    "thresh_params = {\"ttype\": cv.THRESH_BINARY, 'thresh':thresh, 'value': tvalue}\n",
    "\n",
    "fig, ax = plt.subplots(len(imgs), 2, figsize=(20, 10*len(imgs)))\n",
    "\n",
    "for curr_img_idx, curr_img in enumerate(imgs):\n",
    "    img_edges = find_edges_naive(curr_img, highpass_params, thresh_params)\n",
    "\n",
    "    display_image(curr_img, axes=ax[curr_img_idx, 0], cmap='gray')\n",
    "    ax[curr_img_idx, 0].set_title(imgs_name[curr_img_idx], fontsize=30)\n",
    "    display_image(img_edges, axes=ax[curr_img_idx, 1], cmap='gray')\n",
    "    ax[curr_img_idx, 1].set_title(\"Edges\", fontsize=30)\n",
    "    \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f060dd87",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**\n",
    "\n",
    "- For the original image, the edges/outlines are clearly defined, thanks to the threshold setting values to 0 (white). Consequently, the inner regions of the mountains appear very noisy, and we cannot see the details inside the mountain.\n",
    "\n",
    "- For both of the noise cases, the resulting find_edges_function, due to its not having the ability to remove noise, procduce a very noisy image. This is due to the fact that the gradient-based methods are design to find edges by detecting shifts in the signals, and with noise there is a lot of shifts in a small region of pixels. What's worse is that now the pixels is set to either the **specified value**, or zero, resulting in a very high contrast image of noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baa1c3b",
   "metadata": {},
   "source": [
    "Now, let's improve this edge detector by adding a low-pass filter before using the function `find_edges_naive()`. \n",
    "\n",
    "\n",
    "\n",
    "**QUESTION:** (/1)\n",
    "\n",
    "In the next cell, code the function `find_edges_better()` with the following operations:\n",
    "- Use the function `apply_lowpass_filter()` \n",
    "- Use the function `find_edges_naive()` on the low-pass filtered image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee405ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_edges_better(img, lowpass_params, highpass_params, thresh_params):\n",
    "    \n",
    "    \"\"\"\n",
    "    Find edges on a given image.\n",
    "    :param image: Image to enhance contrast\n",
    "    :param lowpass_params: Parameters of the low-pass filter\n",
    "    :param highpass_params: Parameters of the high-pass filter\n",
    "    :param thresh_params: Parameters of the thresholding operation\n",
    "    :return: img_edges: Edges of the image\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    img_flt = apply_lowpass_filter(img, lowpass_params)\n",
    "    img_edges = find_edges_naive(img_flt, highpass_params, thresh_params)\n",
    "    return img_edges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b7f039",
   "metadata": {},
   "source": [
    "The next cell tests your implementation with : \n",
    "- a Gaussian filter of size (3x3)\n",
    "- a Sobel filter of size (3x3) in the x-direction \n",
    "- a binary threshold.\n",
    "\n",
    "**QUESTION:** (/3)\n",
    "\n",
    "Comment the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611456d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "imgs = [cv.imread(os.path.join(os.getcwd(),'data', 'matterhorn.jpg'), cv.IMREAD_GRAYSCALE), \n",
    "        cv.imread(os.path.join(os.getcwd(),'data', 'matterhorn_noise1.jpg'), cv.IMREAD_GRAYSCALE),\n",
    "        cv.imread(os.path.join(os.getcwd(),'data', 'matterhorn_noise2.jpg'), cv.IMREAD_GRAYSCALE)]\n",
    "\n",
    "imgs_name = [\"Noiseless\", \"S&P Noise\", \"Gaussian Noise\"]\n",
    "        \n",
    "lowpass_filter_size = 7\n",
    "highpass_filter_size = 3\n",
    "thresh = 50 \n",
    "tvalue = 255\n",
    "\n",
    "lowpass_params = {\"ftype\":\"gaussian\", \"fsize\": lowpass_filter_size}\n",
    "highpass_params = {\"ftype\":\"sobel\", \"direction\":\"x\", \"fsize\": highpass_filter_size}\n",
    "thresh_params = {\"ttype\": cv.THRESH_BINARY, 'thresh':thresh, 'value': tvalue}\n",
    "\n",
    "fig, ax = plt.subplots(len(imgs), 2, figsize=(20, 10*len(imgs)))\n",
    "\n",
    "for curr_img_idx, curr_img in enumerate(imgs):\n",
    "    img_edges = find_edges_better(curr_img, lowpass_params, highpass_params, thresh_params)\n",
    "\n",
    "    display_image(curr_img, axes=ax[curr_img_idx, 0], cmap='gray')\n",
    "    ax[curr_img_idx, 0].set_title(imgs_name[curr_img_idx], fontsize=30)\n",
    "    display_image(img_edges, axes=ax[curr_img_idx, 1], cmap='gray')\n",
    "    ax[curr_img_idx, 1].set_title(\"Edges\", fontsize=30)\n",
    "    \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4983e1e2",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**\n",
    "\n",
    "- For the original image, after being smoothed out by the gaussian filter, we can see less noise in the inner region of the mountains, thanks to the smoothing effect of the low pass filter. Now, vertical edges inside the mountain are easier to see, compared to the naive version.\n",
    "\n",
    "- For the S&P noise, we are still seeing a lot of noise, but I can notice that the noise intensity reduced slighly due to the smoothing effect of the gaussian filter. The contour of the mountain peak can be subtly seen.\n",
    "\n",
    "- For the Gaussian noise image, after using the gaussian filter to smooth out the noise, the sobel filter performs edge detection much better. The vertical contours can be seen clearly.\n",
    "\n",
    "- All edges in 3 cases look slightly thicker than usual because of all the pixels whose values greater than 50 being set to 255."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a85705",
   "metadata": {},
   "source": [
    "Now that you developed some intuition, it is your turn to design the best edge detector for both noisy images. \n",
    "\n",
    "**QUESTION:** (/4)\n",
    "\n",
    "In the next cell, find the combination of low-pass filter, high-pass filter and thresholding operation that gives you the best edges for each noisy image. Justify your choice and comment your results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea23ada6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images\n",
    "imgs = [cv.imread(os.path.join(os.getcwd(),'data', 'matterhorn_noise1.jpg'), cv.IMREAD_GRAYSCALE),\n",
    "        cv.imread(os.path.join(os.getcwd(),'data', 'matterhorn_noise2.jpg'), cv.IMREAD_GRAYSCALE)]\n",
    "\n",
    "imgs_name = [\"S&P Noise\", \"Gaussian Noise\"]\n",
    "\n",
    "# Best detector parameters \n",
    "\n",
    "###################\n",
    "# YOUR CODE HERE\n",
    "#Gaussian image\n",
    "lowpass_params_g = {\"ftype\":\"gaussian\", \"fsize\": 7}\n",
    "highpass_params_g = {\"ftype\":\"sobel\",\"direction\":\"x\", \"fsize\": 3}\n",
    "thresh_params_g = {\"ttype\": cv.THRESH_BINARY,\"thresh\":50, 'value': 255}\n",
    "\n",
    "#S&P image\n",
    "lowpass_params_sp = {\"ftype\":\"median\", \"fsize\": 5}\n",
    "highpass_params_sp = {\"ftype\":\"sobel\",\"direction\": \"x\", \"fsize\": 1}\n",
    "thresh_params_sp = {\"ttype\": cv.THRESH_BINARY+cv.THRESH_OTSU, 'thresh':0, 'value': 255}\n",
    "###################\n",
    "\n",
    "# Edge detection\n",
    "lowpass_params = [lowpass_params_sp, lowpass_params_g]\n",
    "highpass_params = [highpass_params_sp, highpass_params_g]\n",
    "thresh_params = [thresh_params_sp, thresh_params_g]\n",
    "\n",
    "fig, ax = plt.subplots(len(imgs), 2, figsize=(20, 10*len(imgs)))\n",
    "\n",
    "for curr_img_idx, curr_img in enumerate(imgs):\n",
    "    img_edges = find_edges_better(curr_img, lowpass_params[curr_img_idx], highpass_params[curr_img_idx], thresh_params[curr_img_idx])\n",
    "\n",
    "    display_image(curr_img, axes=ax[curr_img_idx, 0], cmap='gray')\n",
    "    ax[curr_img_idx, 0].set_title(imgs_name[curr_img_idx], fontsize=30)\n",
    "    display_image(img_edges, axes=ax[curr_img_idx, 1], cmap='gray')\n",
    "    ax[curr_img_idx, 1].set_title(\"{} - Edges\".format(imgs_name[curr_img_idx]), fontsize=30)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dfd01e",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**\n",
    "- For S&P image, I decide to use median filter size 5, sobel in x direction size 1, and OTSU+BINARY threshold. The choice for median is explained in above cells. As for the increase from median size 3 to median size 5, after experimenting with different values, selecting 3 appears to leave noise significant enough to intefere with the highpass filters, and any number greater than 5 results in the images loosing too much detail. For the highpass filters, the selection is made after carefully observing three potential candidates: \"sobel-x-size1, sobel-y-size-1, and laplacian-size-1\". The laplacian filter appears just slightly more noisy, and the edge quality is not as defined, so it is discarded. As for the sobel-y-size-1, it loses information in the inner region of the mountains (there is a vertical line), so I decide choose sobel-x-size-1 in the end. Additionally, incresing the size of the highpass filter would make the edges thicker and less refined. For the thresholds, choosing binary enhances the contrast, aiding the visibility of detected edges. Choosing OTSU seems to a good strategy after manually testing out many **threshold value**, since it is done automatically.\n",
    "\n",
    "- For Gaussian images, I choose the gaussian size 7, sobel-x-size-3, and the binary-thresh-50-value-255. Choosing the said gaussian with size 7 smooths enough noise while still preserve the details. Increasing or decresing the filter size suffer from either the negative effect. The resulting image still contains noise, but it is the best set of parameters that we can find after experimentation. For the highpass filters, the laplacian does not function well due to the noise remained, resulting in a very noisy, undefined edges. We resort to choosing sobel-x-size-3 because it is the best filter that preserve edge details. Increasing the size introduces noise into the resulting image, and setting the size to 1 fails to capture any edges. For the threshold, the binary choice is explained above, and the value is set manually after trying many ranges, with the goal of highlighting important edge information and not introduce too much noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8593a1c",
   "metadata": {},
   "source": [
    "As you can imagine, the OpenCV library has some edge detection algorithm directly available. One of them is the [Canny edge detector](https://docs.opencv.org/4.x/dd/d1a/group__imgproc__feature.html#ga2a671611e104c093843d7b7fc46d24af). More info [here] (https://docs.opencv.org/4.x/da/d22/tutorial_py_canny.html). \n",
    "\n",
    "**QUESTION:** (/2)\n",
    "\n",
    "In the next cell, implement the function `find_edges_canny()` which takes as input an image and the detector parameters (thresholds for filtering (See [doc](https://docs.opencv.org/4.x/dd/d1a/group__imgproc__feature.html#ga2a671611e104c093843d7b7fc46d24af)), and the filter size) and returns the filtered image. Use the corresponding OpenCV function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd8fc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_edges_canny(img, canny_params):\n",
    "    \"\"\"\n",
    "    Apply a canny filter on a given image.\n",
    "    :param img: Image to enhance contrast\n",
    "    :param th: Range of the filter threshold \n",
    "    :param filter_size: Size of the filter\n",
    "    :return: img_filt: filtered image\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    th_1 = canny_params[\"thresholds\"][0]\n",
    "    th_2 = canny_params[\"thresholds\"][1]\n",
    "    apertureSize = canny_params[\"fsize\"]\n",
    "    img_filt = cv.Canny(img, th_1, th_2, apertureSize)\n",
    "    return img_filt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a04836",
   "metadata": {},
   "source": [
    "The next cell tests your implementation with a filter of size (3x3) and four threshold ranges. \n",
    "\n",
    "**QUESTION:** (/2)\n",
    "\n",
    "What is the effect of each threshold (low/high minimal threshold, low/high maximal threshold) on the filtered image?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad2cc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "img = cv.imread(os.path.join(os.getcwd(),'data', 'matterhorn.jpg'), cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Canny filtering\n",
    "filter_size = 3\n",
    "\n",
    "thresholds = [(0, 10), (0, 600), (10, 300), (250, 300)]\n",
    "\n",
    "fig, ax = plt.subplots(1, len(thresholds), figsize=(10*len(thresholds), 10))\n",
    "\n",
    "for curr_thresh_idx, curr_thresh in enumerate(thresholds):\n",
    "    \n",
    "    canny_params = {\"fsize\":filter_size, \"thresholds\":curr_thresh}\n",
    "    img_canny = find_edges_canny(img, canny_params)\n",
    "    display_image(img_canny, axes=ax[curr_thresh_idx], cmap='gray')\n",
    "    ax[curr_thresh_idx].set_title(\" Range: {}\".format(curr_thresh), fontsize=30)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7510dab",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**\n",
    "\n",
    "- Low/high minimal threshold: For a low minimal threshold, more pixels will be accepted as edges. As we increase the minimal thresholds, any pixels whose gradients is below this value is discarded as edges. Therefore, the higher the minimum threshold goes, the more pixels are discarded from being edges\n",
    "\n",
    "- Low/high maximal threshold: Any pixels whose gradient value is greater than the maximal threshold is surely an edge. As can be seen in (0,10), almost all of the picture is selected as edges. Increasing this number reduces the number of edges, but only to a certain extent, because whether or not a pixel is considered an edge also depends on another criteria: if it is larger than the minimal and smaller than the maximum, if it then connects to a sure edge it will be considered an edge, this explains figure (0,600), and (0,300), when many pixel whose gradients is in the range connects to a sure edges, resulting in a very dense looking images. Another difference between (0,300) and (0,600) is that in the latter image, because the maximal value is high, you see less edges in the inner regions. This is because there are lest sure edges (ones whose gratdients are greater than 600)\n",
    "\n",
    "- For the final range (250,300), minimal value is high enough to discard unwanted edges, while the maximal value is appropriate to select meaningful edges from the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f1d0ea",
   "metadata": {},
   "source": [
    "In the previous cell, you found the edges of the noiseless image. \n",
    "\n",
    "\n",
    "**QUESTION:** (/4)\n",
    "\n",
    "In the next cell, find the thresholds that give the best edges for the images `matterhorn_noise1.jpg` and `matterhorn_noise2.jpg`. Justify your choice, and compare your results with the homemade edge detector you implemented previously. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cf191f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load images\n",
    "imgs = [cv.imread(os.path.join(os.getcwd(),'data', 'matterhorn_noise1.jpg'), cv.IMREAD_GRAYSCALE),\n",
    "        cv.imread(os.path.join(os.getcwd(),'data', 'matterhorn_noise2.jpg'), cv.IMREAD_GRAYSCALE)]\n",
    "\n",
    "imgs_name = [\"S&P Noise\", \"Gaussian Noise\"]\n",
    "\n",
    "# Best detector parameters \n",
    "\n",
    "\n",
    "###################\n",
    "# YOUR CODE HERE\n",
    "fsize_sp = 5\n",
    "th_sp = (250,325)\n",
    "\n",
    "fsize_g = 5\n",
    "th_g = (250,350)\n",
    "\n",
    "canny_params_sp = {\"fsize\":fsize_sp, \"thresholds\":th_sp}\n",
    "canny_params_g = {\"fsize\":fsize_g, \"thresholds\":th_g}    \n",
    "###################\n",
    "\n",
    "# Edge detection\n",
    "canny_params = [canny_params_sp, canny_params_g]\n",
    "\n",
    "fig, ax = plt.subplots(len(imgs), 2, figsize=(20, 10*len(imgs)))\n",
    "\n",
    "for curr_img_idx, curr_img in enumerate(imgs):\n",
    "    img_edges_canny = find_edges_canny(img, canny_params[curr_img_idx])\n",
    "\n",
    "    display_image(curr_img, axes=ax[curr_img_idx, 0], cmap='gray')\n",
    "    ax[curr_img_idx, 0].set_title(imgs_name[curr_img_idx], fontsize=30)\n",
    "    display_image(img_edges_canny, axes=ax[curr_img_idx, 1], cmap='gray')\n",
    "    ax[curr_img_idx, 1].set_title(\"{} - Edges\".format(imgs_name[curr_img_idx]), fontsize=30)\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39a60be",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**\n",
    "\n",
    "For both of the cases, our set parameters: fsize = 3 and th = (250,325) works well in both achieving noise and producing refined edges. Modifying the fsize does not appear (through eye inspection alone though) to affect the end result. As for the minimal threshold, values around 250 seems to work well in removing unwanted edges/details from the resulting images. Decreasing this number introduce redundant edges/noises to the final result. For the maximal threshold, we choose 330 because when we increase it, we notice some edges in the original images being discarded. Any values below 350 and greater than 300 seems to be a good candidate, and we choose 325 as a middle ground between the values. \n",
    "\n",
    "\n",
    "The resulting images can be seen detecting almost all edges, even the small details in between regions where out find_edges_better method fails. The edges detected using canny are also much sharper and more well defined as compared to our version.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b91daaf",
   "metadata": {},
   "source": [
    "## 1.3 Introduction to Histogram of Gradient (*HoG*)\n",
    "\n",
    "(30 points)\n",
    "\n",
    "### 1.3.1 Introduction\n",
    "\n",
    "Histogram of Oriented Gradients (*HOGs*) are descriptors mainly used in computer vision and machine learning for object detection. HOG features were first introduced by Dalal and Triggs in their CVPR 2005 paper: \"Histogram of Oriented Gradients for Human Detection\" (<http://ieeexplore.ieee.org/document/1467360/>); which transforms image pixels into a vector representation that is sensitive to broadly informative image features regardless of confounding factors like illumination. Later on, we will use these descriptors for detection and tracking. \n",
    "\n",
    "\n",
    "In their work, Dalal and Triggs proposed HOG and a 5-stages descriptor to classify humans in static images.\n",
    "The 5 stages included:\n",
    "\n",
    "    1. Pre-processing and scaling.\n",
    "    2. Computing gradients in both the x and y directions.\n",
    "    3. Obtaining weighted votes in spatial cells (local histograms).\n",
    "    4. Contrast normalizing overlapping spatial cells (Blocks).\n",
    "    5. Collecting all Histograms of Oriented gradients to form the final feature vector.\n",
    "\n",
    "\n",
    "In most real-world applications, HOG is used in conjunction with a [__Linear SVM__](https://en.wikipedia.org/wiki/Support_vector_machine) to perform object detection. HOG rapidly became one of the most used descriptors in image classification. The reason HOG is employed so heavily is because the local object appearance and shape can be characterized using the distribution of local intensity gradients. \n",
    "\n",
    "We’ll be discussing the steps necessary to combine both HOG and a Linear SVM into an object classifier later in this course. But for now, just understand that HOG is mainly used as a descriptor for object detection and that later these descriptors can be fed into a machine learning classifier.\n",
    "\n",
    "\n",
    "### 1.3.2 Objectives of this module\n",
    "\n",
    "HOG based classifiers are already implemented in several methods inside OpenCV. In this module, we will follow the basic steps to construct a feature vector based on HOG from scratch. Once the basic concepts are clear, at the end of this notebook, we will include the OpenCV build-in function used in real-world applications. The parameters of this function should be then clear for you to tune and play with for later applications.\n",
    "\n",
    "*What are HOG descriptors used to describe?*\n",
    "\n",
    "HOG descriptors are mainly used to describe the structural shape and appearance of an object in an image, making them excellent descriptors for object classification. In addition, since HOG captures local intensity gradients and edge directions, it also makes them good as texture descriptors.\n",
    "\n",
    "\n",
    "### 1.3.3 Preprocessing\n",
    "\n",
    "Typically, a feature descriptor converts an image of size $ width \\times height \\times 3 $(channels) to a feature vector array of length $n$. In the case of our implementation of the HOG feature descriptor, the input will be a grayscale image of size 64 x 128 and the output feature vector of a length that will depend on various choices that we will make along the notebook. \n",
    "\n",
    "Before any computation, we must ensure the input image has the right shape. \n",
    "\n",
    "**QUESTION:** (/2)\n",
    "\n",
    "In the next cell, convert the image `lena.png` into the desired format, i.e. a grayscale image of size (64x128)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8d393b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads base image\n",
    "img = cv.imread(os.path.join(os.getcwd(),'data', 'lena.png'))\n",
    "print('Input image has dimensions: {}'.format(img.shape))\n",
    "\n",
    "# converts to grayscale and resize\n",
    "\n",
    "###################\n",
    "# YOUR CODE HERE \n",
    "img_gray= cv.resize(cv.cvtColor(img, cv.COLOR_BGR2GRAY), (64, 128))\n",
    "###################\n",
    "\n",
    "print('Final image has dimensions: {}'.format(img_gray.shape))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "display_image(img, axes=ax[0])\n",
    "ax[0].set_title(\"Original image\", fontsize=30)\n",
    "display_image(img_gray, axes=ax[1], cmap='gray')\n",
    "ax[1].set_title(\"Image for HoG\", fontsize=30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf6a529",
   "metadata": {},
   "source": [
    "### 1.3.4 Gradient computation\n",
    "\n",
    "https://en.wikipedia.org/wiki/Image_gradient\n",
    "\n",
    "Gradient vectors (or “image gradients”) are one of the most fundamental concepts in computer vision; many vision algorithms involve computing gradient vectors for each pixel in an image. A gradient vector it’s simply a measure of the change in pixel values along the x-direction and the y-direction around each pixel. The magnitude is defined as:\n",
    "\n",
    "$$\n",
    "G(x,y) = \\sqrt{(\\Delta x^2 + \\Delta y^2)}\n",
    "$$\n",
    "\n",
    "and the phase: \n",
    "\n",
    "$$\n",
    "\\theta(x,y) = atan(\\frac{\\Delta x}{\\Delta y})\n",
    "$$\n",
    "\n",
    "Where: $\\Delta x = f(x+1,y) - f(x-1,y)$ and $\\Delta y = f(x,y+1) - f(x,y-1)$, are simply the directional change from one pixel to the other.\n",
    "\n",
    "The simplest implementation of the aforementioned operators computes each value by using a mask operator: $-1|0|1$ over the pixel position in each direction. However the Sobel operator you implemented in Section 1.2.2is usually employed as it is more robust to intensity changes.\n",
    "\n",
    "**QUESTION** (/2)\n",
    "\n",
    "In the next cell, use the function `apply_sobel_filter` on the image `img_gray` to compute the gradient of the image in the `x` and `y` directions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7261de",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_size = 1\n",
    "\n",
    "# Sobel filtering\n",
    "\n",
    "###################\n",
    "# YOUR CODE HERE\n",
    "gx = apply_sobel_filter(img_gray, 'x', filter_size)\n",
    "gy = apply_sobel_filter(img_gray, 'y', filter_size)\n",
    "###################\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "obj = display_image(gx, axes=ax[0])\n",
    "ax[0].set_title('Derivative along X direction', fontsize=30)\n",
    "fig.colorbar(obj, orientation ='vertical', ax=ax[0])\n",
    "obj = display_image(gy, axes=ax[1])\n",
    "ax[1].set_title('Derivative along Y direction', fontsize=30)\n",
    "fig.colorbar(obj, orientation ='vertical', ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813eb1fe",
   "metadata": {},
   "source": [
    "**QUESTION**: (/2)\n",
    "- Do the images above make sense?  Explain what each image displayed. \n",
    "- What do the brightest or darkest pixels mean for each direction?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca3de7d",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**\n",
    "\n",
    "- Yes, it makes sense. The one on the left (plotting the derivative along x direction) displays the changes in pixel values on the x axis, so we can clearly see vertical objects that doesn't match with the background colour. It is the same for the other plot but displaying the variations along the y direction => emphasizes horizontal lines. Darker and brighter features that we can see on both pictures emphasize the area where the pixels change while areas with the same shade of gray around to 0 are homogeneous area w.r.t. pixel values. \n",
    "\n",
    "- The colour (shades of gray) evolves as function of the gradient magnitude. The darkest areas represent the strongest negative gradient in the considered direction -x or y- and the brightest ares represent the strongest positive gradient in the considered direction. Positive gradient (bright zones on the plot) means that we move from a dark area of the image to a brighter one while moving towards increasing values on the axis and the reversed way for negative gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e025d1",
   "metadata": {},
   "source": [
    "Since it is a bit hard to distinguish well the previous statements on the picture 'lena.png', we plot here the gradient of an image whose shade of gray only changes along the x axis (horizontally). We can see that the gradient along x is positive as we are moving from a dark area to a brighter one (from left to right) as explained before. The gradient along y is 0 since the colours do not vary along vertical axis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4722280",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_2 = cv.imread(os.path.join(os.getcwd(),'data', 'grad.png'))\n",
    "img_gray_2 = cv.cvtColor(img_2, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "gx_2 = apply_sobel_filter(img_gray_2, 'x', 7)\n",
    "gy_2 = apply_sobel_filter(img_gray_2, 'y', 7)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "obj = display_image(img_gray_2, axes=ax[0])\n",
    "ax[0].set_title('Original image', fontsize=15)\n",
    "\n",
    "obj = display_image(gx_2, axes=ax[1])\n",
    "ax[1].set_title('Derivative along X direction', fontsize=15)\n",
    "fig.colorbar(obj, orientation ='vertical', ax=ax[1])\n",
    "\n",
    "obj = display_image(gy_2, axes=ax[2])\n",
    "ax[2].set_title('Derivative along Y direction', fontsize=15)\n",
    "fig.colorbar(obj, orientation ='vertical', ax=ax[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fee019",
   "metadata": {},
   "source": [
    "As mentionned in the introduction, the images above shows the gradient in a given direction. We can use them to compute the magnitude and phase as in the above formulas.\n",
    "\n",
    "**QUESTION**: (/1)\n",
    "\n",
    "In the next cell, compute the magnitude and the phase (in degrees) of each pixel from the gradient maps `gx` and `gy` computed above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0d7036",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "###################\n",
    "# YOUR CODE HERE\n",
    "magnitude = cv.cartToPolar(gx, gy, angleInDegrees=True)[0] \n",
    "phase = cv.cartToPolar(gx, gy, angleInDegrees=True)[1] \n",
    "###################\n",
    "\n",
    "# Plots of the magnitude and the phase:\n",
    "fig, ax = plt.subplots(1, 2, figsize=(9, 5))\n",
    "\n",
    "#The magnitude is shown in gray scale.\n",
    "obj = display_image(magnitude, axes=ax[0])\n",
    "ax[0].set_title('Gradient Magnitude')\n",
    "fig.colorbar(obj, orientation ='vertical', ax=ax[0])\n",
    "\n",
    "# The phase is shown using a color map. \n",
    "obj = display_image(phase, axes=ax[1], cmap='hsv')\n",
    "ax[1].set_title('Gradient Phase (in degrees)')\n",
    "fig.colorbar(obj, orientation ='vertical', ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131dd2db",
   "metadata": {},
   "source": [
    "From the image above, we see that the phase is computed from 0 to 360 degrees. While this is conceptually correct, two pixels with very similar angular values towards the X-axis (let's say 0.1 radians and 6.2 radians) will be considered very different by a naive classifier. We correct this behavior by wrapping the phase from 0 to $\\pi$ (0 to 180 degrees). At the end of the notebook, you will correct the phase range.\n",
    "\n",
    "\n",
    "**QUESTION:** (/1)\n",
    "\n",
    "From the phase image, we see many red pixels (values very close to zero). Interpret this observation.\n",
    "\n",
    "**Hint:** consider the values of gx and gy in those regions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f932bb",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**\n",
    "\n",
    "The regions where the phase is close to zero correspond to two cases:\n",
    "- 1st (the most common one): $g_x \\rightarrow 0$ and $g_y \\neq 0$. Thus, $\\frac{g_x}{g_y} \\rightarrow 0$, so $atan(\\frac{g_x}{g_y}) \\rightarrow 0$. This means that the change in pixel value along x is negligeable (homogeneous area along x) and changes along y axis.\n",
    "- 2nd: the areas where $g_y$ is much bigger than $g_x$. Thus, $\\frac{g_x}{g_y} \\rightarrow 0$, so $atan(\\frac{g_x}{g_y}) \\rightarrow 0$. This means that the change in pixel values along y axis is much greater than along the x axis which draws the phase to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433a2683",
   "metadata": {},
   "source": [
    "### 1.3.5 Computation of the Histograms of the Phase for Local Cells\n",
    "\n",
    "In the next step, you will compute the histograms of Oriented Gradients, which are histograms of the phase distribution in a region of the image. We split the image into cells, rectangular regions of interest sliding across the image domain. The size and shape of the cell will affect the performance of HOG as a feature vector. For each cell, you will compute a histogram. In this work, you will use $8 px\\times 8px$ cells. \n",
    "\n",
    "In the next cell, we show you the cell subdivisions of the (color) image for cells of size $8\\times 8$ and the histogram of the first cell of the image (starting at the pixel (0, 0)) with 36 bins. The number of bins considered will affect the performance and quality of the descriptor. In the original paper, Dalal and Triggs found that nine bins (from 0 to 180 degrees) performed well for human detection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a13f6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This parameter is usually constant and it's never changed true the procedure.\n",
    "CELL_SIZE = 8;\n",
    "\n",
    "# Histogram of the Phase for the very first cell\n",
    "num_of_bins = 9\n",
    "\n",
    "# Compute histogram\n",
    "hist = cv.calcHist([phase[:CELL_SIZE, :CELL_SIZE].astype(np.float32)], channels=[0], mask=None, histSize=[num_of_bins], ranges=[0,360])\n",
    "\n",
    "# Example of a cell drawn in yellow\n",
    "img_canvas = cv2.resize(img, (64, 128)) # temporal image.\n",
    "\n",
    "for stride_x in range(1,img.shape[0]-1):\n",
    "    for stride_y in range(1,img.shape[1]-1):\n",
    "        cv2.rectangle(img_canvas, (0 + (stride_x-1)*CELL_SIZE, 0 + (stride_y-1)*CELL_SIZE), (0 + stride_x*CELL_SIZE, 0 + stride_y*CELL_SIZE), (0, 255, 255))\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "display_image(img_canvas, axes=ax[0])\n",
    "\n",
    "ax[1].bar(np.arange(hist.shape[0]), hist.reshape(-1), width=1)\n",
    "ax[1].set_xlabel('Orientation')\n",
    "ax[1].set_ylabel('Count')\n",
    "ax[1].set_title('Orientation distribution')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3845b366",
   "metadata": {},
   "source": [
    "The previous cell shows you how to compute the histogram of one cell, but we need the histogram of each cell.\n",
    "\n",
    "**QUESTION:** (/5)\n",
    "\n",
    "In the next cell, implement the function `computeCellHistograms()` that takes as input the phase of the image, the number of bins, and the cell size, and returns a [list](https://docs.python.org/3/tutorial/datastructures.html) containing the histograms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0759e5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCellHistograms(phase, cell_size, num_of_bins):\n",
    "    \"\"\"\n",
    "    Compute histogram for each cells and concatenate them.\n",
    "    \n",
    "    :param phase: 1 channel image with the gradient direction.\n",
    "    :params cell_size: cell size (8px) \n",
    "    :params num_of_bins: number of bins in the histogram\n",
    "    :return concatenated_histograms: list of histograms\n",
    "    \"\"\"\n",
    "    \n",
    "    # Resulting histogram list\n",
    "    concatenated_histograms =[];\n",
    "    # Compute the histogram for each cell\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    for i in range(phase.shape[0]//cell_size):\n",
    "        for j in range(phase.shape[1]//cell_size):\n",
    "            concatenated_histograms.append(cv.calcHist([phase[i*CELL_SIZE:(i+1)*CELL_SIZE, j*CELL_SIZE:(j+1)*CELL_SIZE].astype(np.float32)], \n",
    "                               channels=[0], mask=None, histSize=[num_of_bins], ranges=[0,360]))\n",
    "\n",
    "    return concatenated_histograms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac60e80",
   "metadata": {},
   "source": [
    "The next cell tests your implementation.\n",
    "\n",
    "**QUESTION**: (/1)\n",
    "\n",
    "What should be the number of histograms? Is it correct?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f0be6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute histogram\n",
    "histograms = computeCellHistograms(phase, CELL_SIZE, num_of_bins)\n",
    "\n",
    "print(\"Number of histograms: {}\".format(len(histograms)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf3ac9c",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**\n",
    "\n",
    "There should be 128 histograms since we compute the histograms over 128 cells. We can calculate this number as follows:\n",
    "\n",
    "`number of histograms` = `number of cells on horizontal axis` * `number of cells on vertical axis` = $(\\frac{64}{8})*(\\frac{128}{8}) = 8*16 = 128$ where the number of cells is found by dividing the number of pixels on an axis (64 and 128 for horizontal and vertical respectively) by the cell size (8).\n",
    "\n",
    "So, the number of histograms returned by the previous cell is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a308f8",
   "metadata": {},
   "source": [
    "The function `computeCellHistograms()` computes the histogram of each cell. However, it assumes that each direction has the same importance and contributes equally to the histogram. In the original paper, they proposed to weight the histogram, for instance, using the gradient magnitude. The histogram bins of each cell are weighted by the gradient magnitude. The effect is depicted below:\n",
    "\n",
    "<img src=\"https://gurus.pyimagesearch.com/wp-content/uploads/2015/03/hog_histogram_animation.gif\">\n",
    "<em> Image taken from: https://gurus.pyimagesearch.com </em>\n",
    "\n",
    "The effect of normalizing the weights will directly affect the contribution of each direction.\n",
    "\n",
    "**QUESTION:** (/5)\n",
    "\n",
    "In the next cell, implement the function `computeCellWeightedHistogram()` to return weighted histograms. The new version takes as input the phase of the image, the cell size, the number of bins, and the gradient magnitude and returns a [list](https://docs.python.org/3/tutorial/datastructures.html) containing the histograms. \n",
    "\n",
    "**HINT:** Avoid pre-defined functions to compute the histogram and code your own function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5179bdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCellWeightedHistogram(phase, magnitude, cell_size, num_of_bins):\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute weighted histogram for each cells and concatenate them.\n",
    "    \n",
    "    :param phase: 1 channel image with the gradient direction.\n",
    "    :param magnitude: 1 channel image with the gradient magnitude.\n",
    "    :params cell_size: cell size (8px) \n",
    "    :params num_of_bins: number of bins in the histogram \n",
    "    :return weigthed_and_concatenated_histograms: list of weighted histograms\n",
    "    \"\"\"\n",
    "\n",
    "    weigthed_and_concatenated_histograms = []\n",
    "    \n",
    "    \n",
    "    # CODE HERE\n",
    "    for i in range(phase.shape[0]//cell_size):\n",
    "        for j in range(phase.shape[1]//cell_size): \n",
    "            weigthed_and_concatenated_histograms.append(np.histogram(phase[i*CELL_SIZE:(i+1)*CELL_SIZE, j*CELL_SIZE:(j+1)*CELL_SIZE], \n",
    "                                                        bins=9, range=[0, 360], weights=magnitude[i*CELL_SIZE:(i+1)*CELL_SIZE, j*CELL_SIZE:(j+1)*CELL_SIZE])[0])\n",
    "\n",
    "    weigthed_and_concatenated_histograms = np.array(weigthed_and_concatenated_histograms)\n",
    "    return weigthed_and_concatenated_histograms;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cbf492",
   "metadata": {},
   "source": [
    "The next cell shows you the histogram and the weighted histogram of the first cell. \n",
    "\n",
    "**QUESTION:** (/3)\n",
    "\n",
    "Compare and comment the histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b55dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute weighted histogram\n",
    "weightedHistograms = computeCellWeightedHistogram(phase, magnitude, CELL_SIZE, num_of_bins)\n",
    "\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "ax[0].bar(np.arange(histograms[0].shape[0]), histograms[0].reshape(-1), width=1)\n",
    "ax[0].set_xlabel('Orientation')\n",
    "ax[0].set_ylabel('Count')\n",
    "ax[0].set_title('Orientation distribution')\n",
    "\n",
    "\n",
    "ax[1].bar(np.arange(weightedHistograms[0].shape[0]), weightedHistograms[0].reshape(-1), width=1)\n",
    "ax[1].set_xlabel('Orientation')\n",
    "ax[1].set_ylabel('Count')\n",
    "ax[1].set_title('Orientation distribution')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0cba6b",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**\n",
    "\n",
    "The weigthed histograms have a much greater amplitude than the original ones for certain bins while others have roughly the same amplitude. The distribution seems to be rather the same compared to the original histograms. Since the weighting depends on the magnitude of the gradient the amplitude may increase if the magnitude is bigger than one or decrease if it is less than one. This explains why each bin has not been weighted the same way as the other.\n",
    "\n",
    "The original histogram simply adds up the points where the phase is the same and therefore the overall sum of the bins is equall to 64 since it is the size of the cell. This is not case for the weighted ones as we can see with the next cell where the values have been printed.\n",
    "Bin 7 which is 0 on the first histogram is also 0 on the weighted version which is what we should expect because the weights have no influence if this phase is totally absent in the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35279111",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = [(histograms[0][i], weightedHistograms[0][i]) for i in range(num_of_bins)]\n",
    "\n",
    "print(\"                  Original hist | Weighted hist\")\n",
    "for i in range(num_of_bins):\n",
    "    print('Value of the bin {} is:   {}  |  {}'.format(i, tab[i][0], tab[i][1]))\n",
    "print(\"The sum is:              \", np.sum(histograms[0]), \"|\", np.sum(weightedHistograms[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ae84ba",
   "metadata": {},
   "source": [
    "### 1.3.6 Block normalization\n",
    "\n",
    "In the previous steps, we created the oriented histogram based on the gradient magnitude. However, gradients are sensitive to overall lighting. If you make the image darker by dividing all pixel values by 2, for example, the gradient magnitude will change by half, and, therefore, the histogram values will change by half. Ideally, we want our descriptor to be independent of lighting variations. In other words, we would like to “normalize” the histograms to stabilitze the results. \n",
    "\n",
    "Rather than normalizing each histogram individually, we group the cells into __blocks__ and normalize the histogram of the block instead of the indivual cell. These procedure makes the feature detector less sensitive to intensity changes.\n",
    "\n",
    "\n",
    "<img src = \"https://gurus.pyimagesearch.com/wp-content/uploads/2015/03/hog_contrast_normalization.gif\">\n",
    "<em> Image taken from: https://gurus.pyimagesearch.com </em>\n",
    "\n",
    "Image above shows each block as a group of 4 cells (2 by 2 cells). The blocks overlap each other and, therefore, each cell is represented in the histograms multiple times. This redundancy actually improves accuracy. Finally, we take the resulting block histograms, concatenate them, and treat them as our final feature vector.\n",
    "\n",
    "\n",
    "**QUESTION:** (/6)\n",
    "\n",
    "In the next cell, implement the function `BlockNormalizationAndFeatureConcatenation()` that takes as input the histograms, and the image size and returns the feature vector of histogram normalized with the L2 norm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf849db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BlockNormalizationAndFeatureConcatenation(histograms, img_size, cell_size, NB_BLOCK_CELLS_X = 2, NB_BLOCK_CELLS_Y = 2):\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute histogram  block normalization\n",
    "    \n",
    "    :param histograms: list of weighted histograms\n",
    "    :params img_size:  size of the image\n",
    "    :return \n",
    "    \"\"\"\n",
    "    \n",
    "    feature_vector = []\n",
    "    \n",
    "    #CODE HERE\n",
    "    hist = histograms.tolist()\n",
    "    ## Additoin plus normalisation après\n",
    "    \n",
    "    for i in range(int(img_size[0]/cell_size)-1):\n",
    "        for j in range(int(img_size[1]/cell_size)-1):  \n",
    "            index_1 = int(i*img_size[1]/cell_size)+j\n",
    "            list_hist = np.array(hist[index_1])\n",
    "            \n",
    "            ### Other histograms on the same row\n",
    "            for k in range(1, NB_BLOCK_CELLS_X):\n",
    "                index_k = index_1+k\n",
    "                list_hist += np.array(hist[index_k])\n",
    "                \n",
    "            ### Histgrams on rows below                \n",
    "            for k in range(1, NB_BLOCK_CELLS_Y):\n",
    "                index_k = int((i+k)*img_size[1]/cell_size)+j\n",
    "                list_hist += np.array(hist[index_k])\n",
    "\n",
    "                ### Other histograms on the same row\n",
    "                for m in range(1, NB_BLOCK_CELLS_X):\n",
    "                    index_m = index_k+m\n",
    "                    list_hist+=np.array(hist[index_m])\n",
    "\n",
    "            feature_vector.append(list_hist/np.linalg.norm(list_hist))\n",
    "    feature_vector = np.array(feature_vector)\n",
    "    \n",
    "    return feature_vector    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bc5866",
   "metadata": {},
   "source": [
    "The next cell shows you the histogram of the first cell, the weighted histogram of the first cell and the histogram of the first block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3300ad1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute feature vector\n",
    "feature_vector = BlockNormalizationAndFeatureConcatenation(weightedHistograms, img_gray.shape, CELL_SIZE)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(1, 3, figsize=(24, 8))\n",
    "\n",
    "ax[0].bar(np.arange(histograms[0].shape[0]), histograms[0].reshape(-1), width=1)\n",
    "ax[0].set_xlabel('Orientation')\n",
    "ax[0].set_ylabel('Count')\n",
    "ax[0].set_title('Orientation distribution')\n",
    "\n",
    "ax[1].bar(np.arange(weightedHistograms[0].shape[0]), weightedHistograms[0].reshape(-1), width=1)\n",
    "ax[1].set_xlabel('Orientation')\n",
    "ax[1].set_ylabel('Count')\n",
    "ax[1].set_title('Orientation distribution')\n",
    "\n",
    "ax[2].bar(np.arange(feature_vector[0].shape[0]), feature_vector[0].reshape(-1), width=1)\n",
    "ax[2].set_xlabel('Orientation')\n",
    "ax[2].set_ylabel('Count')\n",
    "ax[2].set_title('Orientation distribution')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faa33b2",
   "metadata": {},
   "source": [
    "### 1.3.7 Feature Vector Visualization\n",
    "\n",
    "The final descriptor is then a 1-dimentional array with the concatenation of the blocks normalized histograms. However, this shouldn't prevent us from having an intuitive visualization of the computed features. \n",
    "\n",
    "**QUESTION:**(/5)\n",
    "\n",
    "As last exercise, you need to display the feature vector over the image space (or any clever way)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13091e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "NB_BLOCK_CELLS_X = 2 \n",
    "NB_BLOCK_CELLS_Y = 2\n",
    "\n",
    "center = []\n",
    "for i in range(int(img_gray.shape[0]/CELL_SIZE)-1):\n",
    "    x = CELL_SIZE*(i+1)\n",
    "    for j in range(int(img_gray.shape[1]/CELL_SIZE)-1):\n",
    "        y = CELL_SIZE*(j+1)\n",
    "        center.append((x, y))\n",
    "        \n",
    "for i, hist in enumerate(feature_vector):\n",
    "    center_x = center[i][0]\n",
    "    center_y = center[i][1]\n",
    "    dx, dy = 0, 0\n",
    "    for j, bar in enumerate(hist):\n",
    "        dx += bar*j*np.cos(np.radians(j*40))\n",
    "        dy += bar*j*np.sin(np.radians(j*40))\n",
    "    ### Attention peut-être inverser x et y\n",
    "    dx *= 0.5\n",
    "    dy *= 0.5\n",
    "    plt.arrow(center_y-0.5*dx, center_x-0.5*dy, dx, dy, width=0.4, color='b')\n",
    "\n",
    "display_image(img_gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1def1cc",
   "metadata": {},
   "source": [
    "### 1.3.8 HOG in action\n",
    "\n",
    "As you may suspect already, OpenCV comes with a nice implementation of the HOG descriptor. Once you finish this notebook, you should be able to understand the basic parameters from the `cv.HOGDescriptor()` class.\n",
    "\n",
    "In the next cell, we use a simple Support Vector Machine algoritmh (SVM) to detect People in multi-scale images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5753db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the HOG descriptor/person detector\n",
    "hog = cv.HOGDescriptor()\n",
    "hog.setSVMDetector(cv.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "# load base image (check that we are not scaling, normalizing or changing the channels)\n",
    "img = cv.imread(os.path.join(os.getcwd(),'data', 'person_104.bmp'))\n",
    "print('Image has dimensions: {}'.format(img.shape))\n",
    "\n",
    "# The HOG detector returns an array with the Regions of maximum likehood to contain a human-shaped-form\n",
    "rects, weights = hog.detectMultiScale(img, winStride=(4, 4), padding=(8, 8), scale=1.05)\n",
    "\n",
    "# draw the original bounding boxes\n",
    "persons = 0;\n",
    "for (x, y, w, h) in rects:\n",
    "    cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, persons * 256), 2)\n",
    "    persons += 1;\n",
    "    \n",
    "display_image(img);\n",
    "plt.title('Detection results')\n",
    "plt.xticks([])\n",
    "plt.yticks([]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739b3ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load base image\n",
    "img = cv.imread(os.path.join(os.getcwd(),'data', 'person_454.bmp'))\n",
    "print('Image has dimensions: {}'.format(img.shape))\n",
    "\n",
    "# The HOG detector returns an array with the Regions of maximum likehood to contain a human-shaped-form\n",
    "rects, weights = hog.detectMultiScale(img , winStride=(4, 4), padding=(8, 8), scale=1.05)\n",
    "\n",
    "# draw the original bounding boxes\n",
    "persons = 0;\n",
    "for (x, y, w, h) in rects:\n",
    "    cv2.rectangle(img , (x, y), (x + w, y + h), (0, 255, persons * 256), 2)\n",
    "    persons += 1;\n",
    "    \n",
    "display_image(img);\n",
    "plt.title('Detection results')\n",
    "plt.xticks([])\n",
    "plt.yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab2862f",
   "metadata": {},
   "source": [
    "### 1.3.9 References\n",
    "\n",
    "https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients\n",
    "\n",
    "https://www.learnopencv.com/histogram-of-oriented-gradients/\n",
    "\n",
    "http://juliaimages.github.io/ImageFeatures.jl/latest/tutorials/object_detection.html\n",
    "\n",
    "https://www.learnopencv.com/tag/hog/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Open CV Lab",
   "language": "python",
   "name": "opencv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
